{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Inlets documentation","text":"<p>Inlets brings secure tunnels to Cloud Native workloads.</p> <p></p> <p>You can visit the inlets homepage at https://inlets.dev/</p> <p>With inlets you are in control of your data, unlike with a SaaS tunnel where shared servers mean your data may be at risk. You can use inlets for local development and in your production environment. It works just as well on bare-metal as in VMs, containers and Kubernetes clusters.</p> <p>inlets is not just compatible with tricky networks and Cloud Native architecture, it was purpose-built for them.</p> <p>Common use-cases include:</p> <ul> <li>Exposing local endpoints on the Internet</li> <li>Self-hosting from a homelab or on-premises datacenter</li> <li>Deploying and monitoring apps across multiple locations</li> <li>Remote customer support</li> </ul> <p>What is your strategy for connecting existing applications to the public cloud? Read: The Simple Way To Connect Existing Apps to Public Cloud.</p>"},{"location":"#how-does-it-work","title":"How does it work?","text":"<p>Inlets tunnels connect to each other over a secure websocket with TLS encryption. Over that private connection, you can then tunnel HTTPS or TCP traffic to computers in another network or to the Internet.</p> <p>One of the most common use-cases is to expose a local HTTP endpoint on the Internet via a HTTPS tunnel. You may be working with webhooks, integrating with OAuth, sharing a draft of a blog post or integrating with a partner's API.</p> <p></p> <p>After deploying an inlets HTTPS server on a public cloud VM, you can then connect the client and access it.</p> <p>There is more that inlets can do for you than exposing local endpoints. inlets also supports local forwarding and can be used to replace more cumbersome services like SSH, complex VPNs or expensive direct connect uplinks.</p> <p>Read more in the: the inlets FAQ.</p>"},{"location":"#getting-started","title":"Getting started","text":"<p>These guides walk you through a specific use-case with inlets. If you have questions or cannot find what you need, there are options for connecting with the community at the end of this page.</p> <p>Inlets can tunnel either HTTP or TCP traffic:</p> <ul> <li>HTTP (L7) tunnels can be used to connect one or more HTTP endpoints from one network to another. A single tunnel can expose multiple websites or hosts, including LoadBalancing and multiple clients to one server.</li> <li>TCP (L4) tunnels can be used to connect TCP services such as a database, a reverse proxy, RDP, Kubernetes or SSH to the Internet. A single tunnel can expose multiple ports on an exit-server and load balance between clients</li> </ul>"},{"location":"#downloading-inlets","title":"Downloading inlets","text":"<p>inlets is available for Windows, MacOS (including M1) and Linux (including ARM):</p> <ul> <li>Download a release</li> </ul> <p>You can also use the container image from ghcr.io: <code>ghcr.io/inlets/inlets-pro:latest</code></p>"},{"location":"#your-first-https-tunnel-with-an-automated-tunnel-server-intermediate","title":"Your first HTTPS tunnel with an automated tunnel server (Intermediate)","text":"<ul> <li>Tutorial: Expose one or more local HTTP services via HTTPS</li> </ul>"},{"location":"#running-a-http-tunnel-server-manually-advanced","title":"Running a HTTP tunnel server manually (Advanced)","text":"<ul> <li>Tutorial: Setting up a HTTP tunnel server manually</li> </ul>"},{"location":"#running-multiple-tunnel-servers-on-the-same-host-advanced","title":"Running multiple tunnel servers on the same host (Advanced)","text":"<p>The easiest way to scale out inlets tunnels is through the Kubernetes helm chart (see below), however you can manually set up a TCP and HTTPS tunnel on the same machine.</p> <ul> <li>Advanced: Setting up dual TCP and HTTPS tunnels</li> </ul>"},{"location":"#local-port-forwarding-intermediate","title":"Local port forwarding (Intermediate)","text":"<ul> <li>Case-study: Reliable local port-forwarding from Kubernetes</li> </ul>"},{"location":"#connecting-with-kubernetes","title":"Connecting with Kubernetes","text":"<p>You may have an on-premises Kubernetes cluster that needs ingress. Perhaps you have a homelab, or Raspberry Pi cluster, that you want to self host services on.</p> <ul> <li>Tutorial: Expose a local IngressController with the inlets-operator</li> <li>Tutorial: Expose Kubernetes services in short-lived clusters with helm</li> </ul> <p>Some teams want to have dev work like production, with tools Istio working locally just like in the cloud.</p> <ul> <li>Tutorial: Expose an Istio gateway with the inlets-operator</li> </ul> <p>See also: helm charts</p>"},{"location":"#tunnelling-tcp-services","title":"Tunnelling TCP services","text":"<p>inlets is not limited to HTTP connections, you can also tunnel TCP protocols like RDP, VNC, SSH, TLS and databases.</p> <ul> <li>Tutorial: Expose a private SSH server over a TCP tunnel</li> <li>Tutorial: Tunnel a private Postgresql database</li> </ul>"},{"location":"#monitoring-and-metrics","title":"Monitoring and metrics","text":"<p>Inlets offers you multiple options to monitor your tunnels and get insight in their performance. Find out tunnel statistics, uptime and connected clients with the <code>inlets-pro status</code> command or collect the Prometheus metrics from the monitoring endpoint.</p> <ul> <li>Monitoring and metrics</li> </ul>"},{"location":"#reference-documentation","title":"Reference documentation","text":""},{"location":"#inletsctl","title":"inletsctl","text":"<p>Learn how to use inletsctl to provision tunnel servers on various public clouds.</p> <ul> <li>inletsctl reference</li> </ul>"},{"location":"#inlets-operator","title":"inlets-operator","text":"<p>Learn how to set up the inlets-operator for Kubernetes, which provisions public cloud VMs and gives IP addresses to your public LoadBalancers.</p> <ul> <li>inlets-operator reference</li> </ul>"},{"location":"#other-resources","title":"Other resources","text":"<p>For news, use-cases and guides check out the blog:</p> <ul> <li>Official Inlets blog</li> </ul> <p>Watch a video, or read a blog post from the community:</p> <ul> <li>Community tutorials</li> </ul> <p>Open Source tools for managing inlets tunnels:</p> <ul> <li>Inlets Operator for Kubernetes LoadBalancers</li> <li>inletsctl to provision tunnel servers</li> <li>inlets helm charts for clients and servers</li> </ul>"},{"location":"#connecting-with-the-inlets-community","title":"Connecting with the inlets community","text":"<p>Who built inlets? Inlets \u00ae is a commercial solution developed and supported by OpenFaaS Ltd.</p> <p>You can also contact the team via the contact page.</p> <p>The code for this website is open source and available on GitHub</p> <p>inlets is proud to be featured on the Cloud Native Landscape in the Service Proxy category.</p> <p></p>"},{"location":"reference/","title":"Reference documentation","text":""},{"location":"reference/#inletsctl","title":"inletsctl","text":"<p>Learn how to use inletsctl to provision tunnel servers on various public clouds.</p> <ul> <li>inletsctl reference</li> </ul>"},{"location":"reference/#inlets-operator","title":"inlets-operator","text":"<p>Learn how to set up the inlets-operator for Kubernetes, which provisions public cloud VMs and gives IP addresses to your public LoadBalancers.</p> <ul> <li>inlets-operator reference</li> </ul>"},{"location":"reference/#github-repositories","title":"GitHub repositories","text":"<ul> <li>inlets-pro</li> <li>inlets-operator</li> <li>inletsctl</li> <li>inlets helm charts</li> </ul>"},{"location":"reference/faq/","title":"Inlets FAQ","text":"<p>Inlets concepts and Frequently Asked Questions (FAQ)</p>"},{"location":"reference/faq/#why-did-we-build-inlets","title":"Why did we build inlets?","text":"<p>We built inlets to make it easy to expose a local service on the Internet and to overcome limitations with SaaS tunnels and VPNs. </p> <ul> <li>It was built to overcome limitations in SaaS tunnels - such as lack of privacy, control and rate-limits</li> <li>It doesn't just integrate with containers and Kubernetes, it was purpose-built to run in them</li> <li>It's easy to run on Windows, Linux and MacOS with a self-contained binary</li> <li>It doesn't need to run as root, doesn't depend on iptables, doesn't need a tun device or NET_ADMIN capability</li> </ul> <p>There are many different networking tools available such as VPNs and SaaS tunnels - each with its own set of pros and cons, and use-cases. It's very likely that you will use several tools together to get the best out of each of them.</p>"},{"location":"reference/faq/#how-does-inlets-compare-to-other-tools-and-solutions","title":"How does inlets compare to other tools and solutions?","text":"<p>Are you curious about the advantages of using inlets vs. alternatives? We must first ask, advantages vs. what other tool or service.</p> <p>SaaS tunnels provide a convenient way to expose services for the purposes of development, however they are often:</p> <ul> <li>blocked by corporate IT</li> <li>running on shared infrastructure (servers) with other customers</li> <li>subject to stringent rate-limits that affect productivity</li> <li>priced per subdomain</li> <li>unable to obtain high value TCP ports like 22, 80, 443 and so on</li> </ul> <p>You run inlets on your own servers, so you do not run into those restrictions. Your data remains your own and is kept private.</p> <p>When compared to VPNs such as Wireguard, Tailscale and OpenVPN, we have to ask what the use-case is.</p> <p>A traditional VPN is built to connect hosts and entire IP ranges together. This can potentially expose a large number of machines and users to each other and requires complex Access Control Lists or authorization rules. If this is your use-case, a traditional VPN is probably the right tool for the job.</p> <p>Inlets is designed to connect or expose services between networks - either HTTP or TCP.</p> <p>For example:</p> <ul> <li>Receiving webhooks to a local application</li> <li>Sharing a blog post draft with a colleague or client</li> <li>Providing remote access to your homelab when away from home</li> <li>Self-hosting websites or services on Kubernetes clusters</li> <li>Getting working LoadBalancers with public IPs for local Kubernetes clusters</li> </ul> <p>You can also use inlets to replace Direct Connect or a VPN when you just need to connect a number of services privately and not an entire network range.</p> <p>Many of the inlets community use a VPN alongside inlets, because they are different tools for different use-cases.</p> <p>We often write about use-cases for public and private inlets tunnels on the blog.</p>"},{"location":"reference/faq/#whats-the-difference-between-inlets-inletsctl-and-inlets-operator","title":"What's the difference between inlets, inletsctl and inlets-operator?","text":"<p>inlets-pro aka \"inlets\" is the command-line tool that contains both the client and server required to set up HTTP and TCP tunnels.</p> <p>The inlets-pro server is usually set up on a computer with a public IP address, then the inlets-pro client is run on your own machine, or a separate computer that can reach the service or server you want to expose.</p> <p>You can download inlets-pro and inletsctl with the \"curl | sh\" commands provided at the start of each tutorial, this works best on a Linux host, or with Git Bash if using Windows.</p> <p>Did you know? You can also download binaries for inlets-pro and inletsctl on GitHub, for Windows users you'll want \"inlets-pro.exe\" and for MacOS, you'll want \"inlets-pro-darwin\".</p> <p>For instance, on Windows machines you'll need \"inlets-pro.exe\"</p> <p>See also: inlets-pro releases</p> <p>inletsctl is a tool that can set up a tunnel server for you on around a dozen popular clouds. It exists to make it quicker and more convenience to set up a HTTPS or TCP tunnel to expose a local service.</p> <p>It has three jobs:</p> <p>1) Create the VM for you 2) Install the inlets-pro server in TCP or HTTPS mode (as specified) with systemd 3) Inform you of the token and connection string</p> <p>You can download the inletsctl tool with \"curl | sh\" or from the inletsctl releases page.</p> <p>Find out more: inletsctl reference page</p> <p>inlets-operator is a Kubernetes Operator that will create tunnel servers for you, on your chosen cloud for any LoadBalancers that you expose within a private cluster.</p> <p>Find out more: inlets-operator reference page</p>"},{"location":"reference/faq/#what-is-the-networking-model-for-inlets","title":"What is the networking model for inlets?","text":"<p>Whilst some networking tools such as Bittorrent use a peer-to-peer network, inlets uses a more traditional client/server model.</p> <p>One or more client tunnels connect to a tunnel server and advertise which services they are able to provide. Then, whenever the server receives traffic for one of those advertised services, it will forward it through the tunnel to the client. The tunnel client will then forward that on to the service it advertised.</p> <p>The tunnel server may also be referred to as an \"exit\" server because it is the connection point for the client to another network or the Internet.</p> <p>If you install and run the inlets server on a computer, it can be referred to as a tunnel server or exit server. These servers can also be automated through cloud-init, terraform, or tools maintained by the inlets community such as inletsctl.</p> <p></p> <p>Pictured: the website <code>http://127.0.0.1:3000</code> is exposed through an encrypted tunnel to users at: <code>https://example.com</code></p> <p>For remote forwarding, the client tends to be run within a private network, with an <code>--upstream</code> flag used to specify where incoming traffic needs to be routed. The tunnel server can then be run on an Internet-facing network, or any other network reachable by the client.</p>"},{"location":"reference/faq/#what-kind-of-layers-and-protocols-are-supported","title":"What kind of layers and protocols are supported?","text":"<p>Inlets works at a higher level than traditional VPNs because it is designed to connect services together, rather than hosts directly.</p> <ul> <li>HTTP - Layer 7 of the OSI model, used for web traffic such as websites and RESTful APIs</li> <li>TCP - Layer 4 of the OSI model, used for TCP traffic like SSH, TLS, databases, RDP, etc</li> </ul> <p>Because VPNs are designed to connect hosts together over a shared IP space, they also involve tedious IP address management and allocation.</p> <p>Inlets connects services, so for TCP traffic, you need only think about TCP ports.</p> <p>For HTTP traffic, you need only to think about domain names.</p>"},{"location":"reference/faq/#do-i-want-a-tcp-or-https-tunnel","title":"Do I want a TCP or HTTPS tunnel?","text":"<p>If you're exposing websites, blogs, docs, APIs and webhooks, you should use a HTTPS tunnel.</p> <p>For HTTP tunnels, Rate Error and Duration (RED) metrics are collected for any service you expose, even if it doesn't have its own instrumentation support.</p> <p>For anything that doesn't fit into that model, a TCP tunnel may be a better option.</p> <p>Common examples are: RDP, VNC, SSH, TLS, database protocols, legacy medical protocols such as DiCom.</p> <p>TCP tunnels can also be used to forward traffic to a reverse proxy like Nginx, Caddy, or Traefik, sitting behind a firewall or NAT by forwarding port 80 and 443.</p> <p>TCP traffic is forwarded directly between the two hosts without any decryption of bytes. The active connection count and frequency can be monitored along with the amount of throughput.</p>"},{"location":"reference/faq/#does-inlets-use-tcp-or-udp","title":"Does inlets use TCP or UDP?","text":"<p>Inlets uses a websocket over TCP, so that it can penetrate HTTP proxies, captive portals, firewalls, and other kinds of NAT. As long as the client can make an outbound connection, a tunnel can be established. The use of HTTPS means that inlets will have similar latency and throughput to a HTTPS server or SSH tunnel.</p> <p>Once you have an inlets tunnel established, you can use it to tunnel traffic to TCP and HTTPS sockets within the private network of the client.</p> <p>Most VPNs tend to use UDP for communication due to its low overhead which results in lower latency and higher throughput. Certain tools and products such as OpenVPN, SSH and Tailscale can be configured to emulate a TCP stack over a TCP connection, this can lead to unexpected issues.</p> <p>Inlets connections send data, rather than emulating a TCP over TCP stack, so doesn't suffer from this problem.</p>"},{"location":"reference/faq/#are-both-remote-and-local-forwarding-supported","title":"Are both remote and local forwarding supported?","text":"<p>Remote forwarding is where a local service is forwarded from the client's network to the inlets tunnel server.</p> <p></p> <p>Remote forwarding pushes a local endpoint to a remote host for access on another network</p> <p>This is the most common use-case and would be used to expose a local HTTP server to the public Internet via a tunnel server.</p> <p>Local forwarding is used to forward a service on the tunnel server or tunnel server's network back to the client, so that it can be accessed using a port on localhost.</p> <p></p> <p>Local forwarding brings a remote service back to localhost for accessing</p> <p>An example would be that you have a webserver and MySQL database. The HTTP server is public and can access the database via its own loopback adapter, but the Internet cannot. So how do you access that MySQL database from CI, or from your local machine? Connect a client with local forwarding, and bring the MySQL port back to your local machine or CI runner, and then use the MySQL CLI to access it.</p> <p>A developer at the UK Government uses inlets to forward a NATS message queue from a staging environment to his local machine for testing. Learn more</p>"},{"location":"reference/faq/#whats-the-difference-between-the-data-plane-and-control-plane","title":"What's the difference between the data plane and control plane?","text":"<p>The data plane is any service or port that carries traffic from the tunnel server to the tunnel client, and your private TCP or HTTP services. It can be exposed on all interfaces, or only bound to loopback for private access, in a similar way to a VPN.</p> <p>If you were exposing SSH on an internal machine from port <code>2222</code>, your data-plane may be exposed on port <code>2222</code></p> <p>The control-plane is a TLS-encrypted, authenticated websocket that is used to connect clients to servers. All traffic ultimately passes over the control-plane's link, so remains encrypted and private.</p> <p>Your control-plane's port is usually <code>8123</code> when used directly, or <code>443</code> when used behind a reverse proxy or Kubernetes Ingress Controller.</p> <p>An example from the article: The Simple Way To Connect Existing Apps to Public Cloud</p> <p>A legacy MSSQL server runs on Windows Server behind the firewall in a private datacenter. Your organisation cannot risk migrating it to an AWS EC2 instance at this time, but can move the microservice that needs to access it.</p> <p>The inlets tunnel allows for the MSSQL service to be tunneled privately to the EC2 instance's local network for accessing, but is not exposed on the Internet. All traffic is encrypted over the wire due to the TLS connection of inlets.</p> <p></p> <p>Hybrid Cloud in action using an inlets tunnel to access the on-premises database</p> <p>This concept is referred to as a a \"split plane\" because the control plane is available to public clients on all adapters, and the data plane is only available on local or private adapters on the server.</p>"},{"location":"reference/faq/#is-there-a-reference-guide-to-the-cli","title":"Is there a reference guide to the CLI?","text":"<p>The inlets-pro binary has built-in help commands and examples, just run <code>inlets-pro tcp/http client/server --help</code>.</p> <p>A separate CLI reference guide is also available here: inlets-pro CLI reference</p>"},{"location":"reference/faq/#is-inlets-secure","title":"Is inlets secure?","text":"<p>All traffic sent over an inlets tunnel is encapsulated in a TLS-encrypted websocket, which prevents eavesdropping. This is technically similar to HTTPS, but you'll see a URL of <code>wss://</code> instead.</p> <p>The tunnel client is authenticated using an API token which is generated by the tunnel administrator, or by automated tooling.</p> <p>Additional authentication mechanisms can be set up using a reverse proxy such as Nginx.</p>"},{"location":"reference/faq/#do-i-have-to-expose-services-on-the-internet-to-use-inlets","title":"Do I have to expose services on the Internet to use inlets?","text":"<p>No, inlets can be used to tunnel one or more services to another network without exposing them on the Internet.</p> <p>The <code>--data-addr 127.0.0.1:</code> flag for inlets servers binds the data plane to the server's loopback address, meaning that only other processing running on it can access the tunneled services. You could also use a private network adapter or VPC IP address in the <code>--data-addr</code> flag.</p>"},{"location":"reference/faq/#how-do-i-monitor-inlets","title":"How do I monitor inlets?","text":"<p>See the following blog post for details on the <code>inlets status</code> command and the various Prometheus metrics that are made available.</p> <p>Measure and monitor your inlets tunnels</p>"},{"location":"reference/faq/#how-do-you-scale-inlets","title":"How do you scale inlets?","text":"<p>Inlets HTTP servers can support a high number of clients, either for load-balancing the same internal service to a number of clients, or for a number of distinct endpoints.</p> <p>Tunnel servers are easy to scale through the use of containers, and can benefit from the resilience that a Kubernetes cluster can bring:</p> <p>See also: How we scaled inlets to thousands of tunnels with Kubernetes</p>"},{"location":"reference/faq/#does-inlets-support-high-availability-ha","title":"Does inlets support High Availability (HA)?","text":"<p>For the inlets client, it is possible to connect multiple inlets tunnel clients for the same service, such as a company blog. Traffic will be distributed across the clients and if one of those clients goes down or crashes, the other will continue to serve requests.</p> <p>For the inlets tunnel server, the easiest option is to run the server in a supervisor that can restart the tunnel service quickly or allow it to run more than one replica. Systemd can be used to restart tunnel servers should they run into issues, likewise you can run the server in a container, or as a Kubernetes Pod.</p> <p></p> <p>HA example with an AWS ELB</p> <p>For example, you may place a cloud load-balancer in front of the data-plane port of two inlets server processes. Requests to the stable load-balancer IP address will be distributed between the two virtual machines and their respective inlets server tunnel processes.  </p>"},{"location":"reference/faq/#is-ipv6-supported","title":"Is IPv6 supported?","text":"<p>Yes, see also: How to serve traffic to IPv6 users with inlets</p>"},{"location":"reference/faq/#what-if-the-websocket-disconnects","title":"What if the websocket disconnects?","text":"<p>The client will reconnect automatically and can be configured with systemd or a Windows service to stay running in the background. See also <code>inlets pro tcp/http server/client --generate=systemd</code> for generating systemd unit files.</p> <p>When used in combination with a Kubernetes ingress controller or reverse proxy of your own, then the websocket may timeout. These timeout settings can usually be configured to remove any potential issue.</p> <p>Monitoring in inlets allows for you to monitor the reliability of your clients and servers, which are often running in distinct networks.</p>"},{"location":"reference/faq/#how-much-does-inlets-cost","title":"How much does inlets cost?","text":"<p>Monthly and annual subscriptions are available via Gumroad.</p> <p>You can also purchase a static license for offline or air-gapped environments.</p> <p>For more, see the Pricing page</p>"},{"location":"reference/faq/#what-happens-when-the-license-expires","title":"What happens when the license expires?","text":"<p>If you're using a Gumroad license, and keep your billing relationship active, then the software will work for as long as you keep paying. The Gumroad license server needs to be reachable by the inlets client.</p> <p>If you're using a static license, then the software will continue to run, even after your license has expired, unless you restart the software. You can either rotate the token on your inlets clients in an automated or manual fashion, or purchase a token for a longer period of time up front.</p>"},{"location":"reference/faq/#can-i-get-professional-help","title":"Can I get professional help?","text":"<p>Inlets is designed to be self-service and is well documented, but perhaps you could use some direction?</p> <p>Business licenses come with support via email, however you are welcome to contact OpenFaaS Ltd to ask about a consulting project.</p>"},{"location":"reference/inlets-operator/","title":"inlets-operator reference documentation","text":"<p>The inlets/inlets-operator brings LoadBalancers with public IP addresses to your local Kubernetes clusters.</p> <p>It works by creating VMs and running an inlets Pro tunnel server for you, the VM's public IP is then attached to the cluster and an inlets client Pod runs for you.</p> <p>You can install the inlets-operator using a single command with arkade or with helm. arkade is an open-source Kubernetes marketplace and easier to use.</p> <p>For each provider, the minimum requirements tend to be:</p> <ul> <li>An access token - for the operator to create VMs for inlets Pro servers</li> <li>A region - where to create the VMs</li> </ul> <p>You can subscribe to inlets for personal or commercial use via Gumroad</p>"},{"location":"reference/inlets-operator/#install-using-arkade","title":"Install using arkade","text":"<pre><code>arkade install inlets-operator \\\n--provider $PROVIDER \\ # Name of the cloud provider to provision the exit-node on.\n--region $REGION \\ # Used with cloud providers that require a region.\n--zone $ZONE \\ # Used with cloud providers that require zone (e.g. gce).\n--token-file $HOME/Downloads/key.json \\ # Token file/Service Account Key file with the access to the cloud provider.\n--license-file $HOME/.inlets/LICENSE\n</code></pre>"},{"location":"reference/inlets-operator/#install-using-helm","title":"Install using helm","text":"<p>Checkout the inlets-operator helm chart README to know more about the values that can be passed to <code>--set</code> and to see provider specific example commands.</p> <pre><code># Create a secret to store the service account key file\nkubectl create secret generic inlets-access-key \\\n--from-file=inlets-access-key=key.json\n\n# Add and update the inlets-operator helm repo\nhelm repo add inlets https://inlets.github.io/inlets-operator/\n\n# Create a namespace for inlets-operator\nkubectl create namespace inlets\n\n# Create a secret to store the inlets-pro license\nkubectl create secret generic -n inlets \\\ninlets-license --from-file license=$HOME/.inlets/LICENSE\n\n# Update the local repository\nhelm repo update\n\n# Install inlets-operator with the required fields\nhelm upgrade inlets-operator --install inlets/inlets-operator \\\n--set provider=$PROJECTID,zone=$ZONE,region=$REGION \\\n--set projectID=$PROJECTID \\\n--set inletsProLicense=$LICENSE\n</code></pre> <p>View the code and chart on GitHub: inlets/inlets-operator</p>"},{"location":"reference/inlets-operator/#instructions-per-cloud","title":"Instructions per cloud","text":""},{"location":"reference/inlets-operator/#create-tunnel-servers-on-digitalocean","title":"Create tunnel servers on DigitalOcean","text":"<p>Install with inlets Pro on DigitalOcean.</p> <p>Assuming you have created an API key and saved it to <code>$HOME/Downloads/do-access-token</code>, run:</p> <pre><code>arkade install inlets-operator \\\n--provider digitalocean \\\n--region lon1 \\\n--token-file $HOME/Downloads/do-access-token \\\n--license-file $HOME/.inlets/LICENSE\n</code></pre>"},{"location":"reference/inlets-operator/#create-tunnel-servers-on-aws-ec2","title":"Create tunnel servers on AWS EC2","text":"<p>Instructions for AWS EC2</p> <p>To use the instructions below you must have the AWS CLI configured with sufficient permissions to create users and roles.</p> <ul> <li>Create a AWS IAM Policy with the following:</li> </ul> <p>Create a file named <code>policy.json</code> with the following content</p> <pre><code>{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n{\n\"Effect\": \"Allow\",\n\"Action\": [\n\"ec2:AuthorizeSecurityGroupIngress\",\n\"ec2:DescribeInstances\",\n\"ec2:DescribeImages\",\n\"ec2:TerminateInstances\",\n\"ec2:CreateSecurityGroup\",\n\"ec2:CreateTags\",\n\"ec2:DeleteSecurityGroup\",\n\"ec2:RunInstances\",\n\"ec2:DescribeInstanceStatus\"\n],\n\"Resource\": [\"*\"]\n}\n]\n}\n</code></pre> <p>Create the policy in AWS</p> <pre><code>aws iam create-policy --policy-name inlets-automation --policy-document file://policy.json\n</code></pre> <ul> <li>Create an IAM user</li> </ul> <pre><code>aws iam create-user --user-name inlets-automation\n</code></pre> <ul> <li>Add the Policy to the IAM user</li> </ul> <p>We need to use the policy arn generated above, it should have been printed to the console on success. It also follows the format below.</p> <pre><code>export AWS_ACCOUNT_NUMBER=\"Your AWS Account Number\"\naws iam attach-user-policy --user-name inlets-automation --policy-arn arn:aws:iam::${AWS_ACCOUNT_NUMBER}:policy/inlets-automation\n</code></pre> <ul> <li>Generate an access key for your IAM User</li> </ul> <p>The below commands will create a set of credentials and save them into files for use later on.</p> <p>we are using jq here. It can be installed using the link provided. Alternatively you can print ACCESS_KEY_JSON and create the files manually.</p> <pre><code>ACCESS_KEY_JSON=$(aws iam create-access-key --user-name inlets-automation)\necho $ACCESS_KEY_JSON | jq -r .AccessKey.AccessKeyId &gt; access-key\necho $ACCESS_KEY_JSON | jq -r .AccessKey.SecretAccessKey &gt; secret-access-key\n</code></pre> <p>Install with inlets Pro:</p> <pre><code>arkade install inlets-operator \\\n--provider ec2 \\\n--region eu-west-1 \\\n--token-file $HOME/Downloads/access-key \\\n--secret-key-file $HOME/Downloads/secret-access-key \\\n--license-file $HOME/.inlets/LICENSE\n</code></pre>"},{"location":"reference/inlets-operator/#create-tunnel-servers-on-google-compute-engine-gce","title":"Create tunnel servers on Google Compute Engine (GCE)","text":"<p>Instructions for Google Cloud</p> <p>It is assumed that you have gcloud installed and configured on your machine. If not, then follow the instructions here</p> <p>To get your service account key file, follow the steps below:</p> <pre><code># Get current projectID\nexport PROJECTID=$(gcloud config get-value core/project 2&gt;/dev/null)\n\n# Create a service account\ngcloud iam service-accounts create inlets \\\n--description \"inlets-operator service account\" \\\n--display-name \"inlets\"\n\n# Get service account email\nexport SERVICEACCOUNT=$(gcloud iam service-accounts list | grep inlets | awk '{print $2}')\n\n# Assign appropriate roles to inlets service account\ngcloud projects add-iam-policy-binding $PROJECTID \\\n--member serviceAccount:$SERVICEACCOUNT \\\n--role roles/compute.admin\n\ngcloud projects add-iam-policy-binding $PROJECTID \\\n--member serviceAccount:$SERVICEACCOUNT \\\n--role roles/iam.serviceAccountUser\n\n# Create inlets service account key file\ngcloud iam service-accounts keys create key.json \\\n--iam-account $SERVICEACCOUNT\n</code></pre> <p>Install the operator:</p> <pre><code>arkade install inlets-operator \\\n--provider gce \\\n--project-id $PROJECTID \\\n--zone us-central1-a \\\n--token-file key.json \\\n--license-file $HOME/.inlets/LICENSE\n</code></pre>"},{"location":"reference/inlets-operator/#create-tunnel-servers-on-azure","title":"Create tunnel servers on Azure","text":"<p>Instructions for Azure</p> <p>Prerequisites:</p> <ul> <li>You will need <code>az</code>. See Install the Azure CLI</li> <li>You'll need to have run <code>az login</code> also</li> </ul> <p>Generate Azure authentication file:</p> <pre><code>SUBSCRIPTION_ID=\"YOUR_SUBSCRIPTION_ID\"\naz ad sp create-for-rbac --role Contributor --scopes \"/subscriptions/$SUBSCRIPTION_ID\" --sdk-auth \\\n&gt; $HOME/Downloads/client_credentials.json\n</code></pre> <p>Find your region code with:</p> <pre><code>az account list-locations -o table\n\nDisplayName               Name                 RegionalDisplayName\n------------------------  -------------------  -------------------------------------\nUnited Kingdom            ukwest               United Kingdom\n</code></pre> <p>Install using helm:</p> <pre><code>export SUBSCRIPTION_ID=\"YOUR_SUBSCRIPTION_ID\"\nexport AZURE_REGION=\"ukwest\"\nexport INLETS_LICENSE=\"$(cat ~/.inlets/LICENSE)\"\nexport ACCESS_KEY=\"$HOME/Downloads/client_credentials.json\"\n\nkubectl create secret generic inlets-access-key \\\n--from-file=inlets-access-key=$ACCESS_KEY\n\nhelm repo add inlets https://inlets.github.io/inlets-operator/\nhelm repo update\n\nhelm upgrade inlets-operator --install inlets/inlets-operator \\\n--set provider=azure,region=$AZURE_REGION \\\n--set subscriptionID=$SUBSCRIPTION_ID\n</code></pre>"},{"location":"reference/inlets-operator/#create-tunnel-servers-on-linode","title":"Create tunnel servers on Linode","text":"<p>Instructions for Linode</p> <p>Install using helm:</p> <pre><code># Create a secret to store the service account key file\nkubectl create secret generic inlets-access-key --from-literal inlets-access-key=&lt;Linode API Access Key&gt;\n\n# Add and update the inlets-operator helm repo\nhelm repo add inlets https://inlets.github.io/inlets-operator/\n\nhelm repo update\n\n# Install inlets-operator with the required fields\nhelm upgrade inlets-operator --install inlets/inlets-operator \\\n--set provider=linode \\\n--set region=us-east\n</code></pre> <p>You can also install the inlets-operator using a single command using arkade, arkade runs against any Kubernetes cluster.</p> <p>Install with inlets Pro:</p> <pre><code>arkade install inlets-operator \\\n--provider linode \\\n--region us-east \\\n--access-key $LINODE_ACCESS_KEY \\\n--license-file $HOME/.inlets/LICENSE\n</code></pre>"},{"location":"reference/inletsctl/","title":"inletsctl reference documentation","text":"<p>inletsctl is an automation tool for inlets/-pro.</p> <p>Features:</p> <ul> <li><code>create</code> / <code>delete</code> cloud VMs with inlets/-pro pre-installed via systemd</li> <li><code>download [--pro]</code> - download the inlets/-pro binaries to your local computer</li> <li><code>kfwd</code> - forward services from a Kubernetes cluster to your local machine using inlets/-pro</li> </ul> <p>View the code on GitHub: inlets/inletsctl</p>"},{"location":"reference/inletsctl/#install-inletsctl","title":"Install <code>inletsctl</code>","text":"<p>You can install inletsctl using its installer, or from the GitHub releases page</p> <pre><code># Install to local directory (and for Windows users)\ncurl -sLSf https://inletsctl.inlets.dev | sh\n\n# Install directly to /usr/local/bin/\ncurl -sLSf https://inletsctl.inlets.dev | sudo sh\n</code></pre> <p>Windows users are encouraged to use git bash to install the inletsctl binary.</p>"},{"location":"reference/inletsctl/#downloading-inlets-pro","title":"Downloading inlets-pro","text":"<p>The <code>inletsctl download</code> command can be used to download the inlets/-pro binaries.</p> <p>Example usage:</p> <pre><code># Download the latest inlets-pro binary\ninletsctl download\n\n# Download a specific version of inlets-pro\ninletsctl download --version 0.8.5\n</code></pre>"},{"location":"reference/inletsctl/#the-create-command","title":"The <code>create</code> command","text":""},{"location":"reference/inletsctl/#create-a-https-tunnel-with-a-custom-domain","title":"Create a HTTPS tunnel with a custom domain","text":"<p>This example uses DigitalOcean to create a cloud VM and then exposes a local service via the newly created exit-server.</p> <p>Let's say we want to expose a Grafana server on our internal network to the Internet via Let's Encrypt and HTTPS?</p> <pre><code>export DOMAIN=\"grafana.example.com\"\n\ninletsctl create \\\n--provider digitalocean \\\n--region=\"lon1\" \\\n--access-token-file $HOME/do-access-token \\\n--letsencrypt-domain $DOMAIN \\\n--letsencrypt-email webmaster@$DOMAIN \\\n--letsencrypt-issuer prod\n</code></pre> <p>You can also use <code>--letsencrypt-issuer</code> with the <code>staging</code> value whilst testing since Let's Encrypt rate-limits how many certificates you can obtain within a week.</p> <p>Create a DNS A record for the IP address so that <code>grafana.example.com</code> for instance resolves to that IP. For instance you could run:</p> <pre><code>doctl compute domain create \\\n--ip-address 46.101.60.161 grafana.example.com\n</code></pre> <p>Now run the command that you were given, and if you wish, change the upstream to point to the domain explicitly:</p> <pre><code># Obtain a license at https://inlets.dev\n# Store it at $HOME/.inlets/LICENSE or use --help for more options\n\n# Where to route traffic from the inlets server\nexport UPSTREAM=\"grafana.example.com=http://192.168.0.100:3000\"\n\ninlets-pro http client --url \"wss://46.101.60.161:8123\" \\\n--token \"lRdRELPrkhA0kxwY0eWoaviWvOoYG0tj212d7Ff0zEVgpnAfh5WjygUVVcZ8xJRJ\" \\\n--upstream $UPSTREAM\n\nTo delete:\n  inletsctl delete --provider digitalocean --id \"248562460\"\n</code></pre> <p>You can also specify more than one domain and upstream for the same tunnel, so you could expose OpenFaaS and Grafana separately for instance.</p> <p>Update the <code>inletsctl create</code> command with multiple domains such as: <code>--letsencrypt-domain openfaas.example.com --letsencrypt-domain grafana.example.com</code></p> <p>Then for the <code>inlets-pro client</code> command, update the upstream in the same way by repeating the flag once per upstream mapping: <code>--upstream openfaas.example.com=http://127.0.0.1:8080</code> <code>--upstream grafana.example.com=http://192.168.0.100:3000</code>.</p> <p>Note that in previous inlets versions, multiple upstream values were given in a single flag, separated by commas, this has now been deprecated for the above syntax.</p>"},{"location":"reference/inletsctl/#create-a-http-tunnel","title":"Create a HTTP tunnel","text":"<p>This example uses Linode to create a cloud VM and then exposes a local service via the newly created exit-server.</p> <pre><code>export REGION=\"eu-west\"\n\ninletsctl create \\\n--provider linode \\\n--region=\"$REGION\" \\\n--access-token-file $HOME/do-access-token\n</code></pre> <p>You'll see the host being provisioned, it usually takes just a few seconds:</p> <pre><code>Using provider: linode\nRequesting host: peaceful-lewin8 in eu-west, from linode\n2021/06/01 15:56:03 Provisioning host with Linode\nHost: 248561704, status: \n[1/500] Host: 248561704, status: new\n...\n[11/500] Host: 248561704, status: active\n\ninlets Pro (0.7.0) exit-server summary:\n  IP: 188.166.168.90\n  Auth-token: dZTkeCNYgrTPvFGLifyVYW6mlP78ny3jhyKM1apDL5XjmHMLYY6MsX8S2aUoj8uI\n</code></pre> <p>Now run the command given to you, changing the <code>--upstream</code> URL to match a local URL such as <code>http://localhost:3000</code></p> <pre><code># Obtain a license at https://inlets.dev\nexport LICENSE=\"$HOME/.inlets/license\"\n\n# Give a single value or comma-separated\nexport PORTS=\"3000\"\n\n# Where to route traffic from the inlets server\nexport UPSTREAM=\"localhost\"\n\ninlets-pro tcp client --url \"wss://188.166.168.90:8123/connect\" \\\n--token \"dZTkeCNYgrTPvFGLifyVYW6mlP78ny3jhyKM1apDL5XjmHMLYY6MsX8S2aUoj8uI\" \\\n--upstream $UPSTREAM \\\n--ports $PORTS\n</code></pre> <p>The client will look for your license in <code>$HOME/.inlets/LICENSE</code>, but you can also use the <code>--license/--license-file</code> flag if you wish.</p> <p>You can then access your local website via the Internet and the exit-server's IP at:</p> <p>http://188.166.168.90</p> <p>When you're done, you can delete the host using its ID or IP address:</p> <pre><code>  inletsctl delete --provider linode --id \"248561704\"\ninletsctl delete --provider linode --ip \"188.166.168.90\"\n</code></pre>"},{"location":"reference/inletsctl/#create-a-tunnel-for-a-tcp-service","title":"Create a tunnel for a TCP service","text":"<p>This example is similar to the previous one, but also adds link-level encryption between your local service and the exit-server.</p> <p>In addition, you can also expose pure TCP traffic such as SSH or Postgresql.</p> <pre><code>inletsctl create \\\n--provider digitalocean \\\n--access-token-file $HOME/do-access-token \\\n--pro\n</code></pre> <p>Note the output:</p> <pre><code>inlets Pro (0.7.0) exit-server summary:\n  IP: 142.93.34.79\n  Auth-token: TUSQ3Dkr9QR1VdHM7go9cnTUouoJ7HVSdiLq49JVzY5MALaJUnlhSa8kimlLwBWb\n\nCommand:\n  export LICENSE=\"\"\nexport PORTS=\"8000\"\nexport UPSTREAM=\"localhost\"\n\ninlets-pro tcp client --url \"wss://142.93.34.79:8123/connect\" \\\n--token \"TUSQ3Dkr9QR1VdHM7go9cnTUouoJ7HVSdiLq49JVzY5MALaJUnlhSa8kimlLwBWb\" \\\n--license \"$LICENSE\" \\\n--upstream $UPSTREAM \\\n--ports $PORTS\n\nTo Delete:\n          inletsctl delete --provider digitalocean --id \"205463570\"\n</code></pre> <p>Run a local service that uses TCP such as MariaDB:</p> <pre><code>head -c 16 /dev/urandom |shasum \n8cb3efe58df984d3ab89bcf4566b31b49b2b79b9\n\nexport PASSWORD=\"8cb3efe58df984d3ab89bcf4566b31b49b2b79b9\"\n\ndocker run --name mariadb \\\n-p 3306:3306 \\\n-e MYSQL_ROOT_PASSWORD=8cb3efe58df984d3ab89bcf4566b31b49b2b79b9 \\\n-ti mariadb:latest\n</code></pre> <p>Connect to the tunnel updating the ports to <code>3306</code></p> <pre><code>export LICENSE=\"$(cat ~/LICENSE)\"\nexport PORTS=\"3306\"\nexport UPSTREAM=\"localhost\"\n\ninlets-pro tcp client --url \"wss://142.93.34.79:8123/connect\" \\\n--token \"TUSQ3Dkr9QR1VdHM7go9cnTUouoJ7HVSdiLq49JVzY5MALaJUnlhSa8kimlLwBWb\" \\\n--license \"$LICENSE\" \\\n--upstream $UPSTREAM \\\n--ports $PORTS\n</code></pre> <p>Now connect to your MariaDB instance from its public IP address:</p> <pre><code>export PASSWORD=\"8cb3efe58df984d3ab89bcf4566b31b49b2b79b9\"\nexport EXIT_IP=\"142.93.34.79\"\n\ndocker run -it --rm mariadb:latest mysql -h $EXIT_IP -P 3306 -uroot -p$PASSWORD\n\nWelcome to the MariaDB monitor.  Commands end with ; or \\g.\nYour MariaDB connection id is 3\nServer version: 10.5.5-MariaDB-1:10.5.5+maria~focal mariadb.org binary distribution\n\nCopyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.\n\nType 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.\n\nMariaDB [(none)]&gt; create database test; \nQuery OK, 1 row affected (0.039 sec)\n</code></pre>"},{"location":"reference/inletsctl/#examples-for-specific-cloud-providers","title":"Examples for specific cloud providers","text":""},{"location":"reference/inletsctl/#example-usage-with-aws-ec2","title":"Example usage with AWS EC2","text":"<p>To use the instructions below you must have the AWS CLI configured with sufficient permissions to  create users and roles. </p> <ul> <li>Create a AWS IAM Policy with the following:</li> </ul> <p>Create a file named <code>policy.json</code> with the following content</p> <pre><code>{\n\"Version\": \"2012-10-17\",\n\"Statement\": [  {\n\"Effect\": \"Allow\",\n\"Action\": [\n\"ec2:AuthorizeSecurityGroupIngress\",\n\"ec2:DescribeInstances\",\n\"ec2:DescribeImages\",\n\"ec2:TerminateInstances\",\n\"ec2:CreateSecurityGroup\",\n\"ec2:CreateTags\",\n\"ec2:DeleteSecurityGroup\",\n\"ec2:RunInstances\",\n\"ec2:DescribeInstanceStatus\"\n],\n\"Resource\": [\"*\"]\n}\n]\n}\n</code></pre> <p>Create the policy in AWS </p> <pre><code>aws iam create-policy --policy-name inlets-automation --policy-document file://policy.json\n</code></pre> <ul> <li>Create an IAM user</li> </ul> <pre><code>aws iam create-user --user-name inlets-automation\n</code></pre> <ul> <li>Add the Policy to the IAM user</li> </ul> <p>We need to use the policy arn generated above, it should have been printed to the console on success. It also follows the format below.</p> <pre><code>export AWS_ACCOUNT_NUMBER=\"Your AWS Account Number\"\naws iam attach-user-policy --user-name inlets-automation --policy-arn arn:aws:iam::${AWS_ACCOUNT_NUMBER}:policy/inlets-automation\n</code></pre> <ul> <li>Generate an access key for your IAM User </li> </ul> <p>The below commands will create a set of credentials and save them into files for use later on.</p> <p>we are using jq here. It can be installed using the link provided. Alternatively you can print ACCESS_KEY_JSON and create the files manually.</p> <pre><code>ACCESS_KEY_JSON=$(aws iam create-access-key --user-name inlets-automation)\necho $ACCESS_KEY_JSON | jq -r .AccessKey.AccessKeyId &gt; access-key.txt\necho $ACCESS_KEY_JSON | jq -r .AccessKey.SecretAccessKey &gt; secret-key.txt\n</code></pre> <ul> <li>Create an exit-server:</li> </ul> <pre><code>inletsctl create \\\n--provider ec2 \\\n--region eu-west-1 \\\n--access-token-file ./access-key.txt \\\n--secret-key-file ./secret-key.txt\n</code></pre> <ul> <li>Delete an exit-server:</li> </ul> <pre><code>export IP=\"\"\n\ninletsctl create \\\n--provider ec2 \\\n--region eu-west-1 \\\n--access-token-file ./access-key.txt \\\n--secret-key-file ./secret-key.txt \\\n--ip $IP\n</code></pre>"},{"location":"reference/inletsctl/#example-usage-with-aws-ec2-temporary-credentials","title":"Example usage with AWS EC2 Temporary Credentials","text":"<p>To use the instructions below you must have the AWS CLI configured with sufficient permissions to  create users and roles. </p> <p>The following instructions use <code>get-session-token</code> to illustrate the concept.  However, it is expected that real world usage would more likely make use of <code>assume-role</code> to obtain temporary credentials.</p> <ul> <li>Create a AWS IAM Policy with the following:</li> </ul> <p>Create a file named <code>policy.json</code> with the following content</p> <pre><code>{\n\"Version\": \"2012-10-17\",\n\"Statement\": [  {\n\"Effect\": \"Allow\",\n\"Action\": [\n\"ec2:AuthorizeSecurityGroupIngress\",\n\"ec2:DescribeInstances\",\n\"ec2:DescribeImages\",\n\"ec2:TerminateInstances\",\n\"ec2:CreateSecurityGroup\",\n\"ec2:CreateTags\",\n\"ec2:DeleteSecurityGroup\",\n\"ec2:RunInstances\",\n\"ec2:DescribeInstanceStatus\"\n],\n\"Resource\": [\"*\"]\n}\n]\n}\n</code></pre> <ul> <li>Create the policy in AWS </li> </ul> <pre><code>aws iam create-policy --policy-name inlets-automation --policy-document file://policy.json\n</code></pre> <ul> <li>Create an IAM user</li> </ul> <pre><code>aws iam create-user --user-name inlets-automation\n</code></pre> <ul> <li>Add the Policy to the IAM user</li> </ul> <p>We need to use the policy arn generated above, it should have been printed to the console on success. It also follows the format below.</p> <pre><code>export AWS_ACCOUNT_NUMBER=\"Your AWS Account Number\"\naws iam attach-user-policy --user-name inlets-automation --policy-arn arn:aws:iam::${AWS_ACCOUNT_NUMBER}:policy/inlets-automation\n</code></pre> <ul> <li>Generate an access key for your IAM User </li> </ul> <p>The below commands will create a set of credentials and save them into files for use later on.</p> <p>we are using jq here. It can be installed using the link provided. Alternatively you can print ACCESS_KEY_JSON and create the files manually.</p> <pre><code>ACCESS_KEY_JSON=$(aws iam create-access-key --user-name inlets-automation)\nexport AWS_ACCESS_KEY_ID=$(echo $ACCESS_KEY_JSON | jq -r .AccessKey.AccessKeyId)\nexport AWS_SECRET_ACCESS_KEY=$(echo $ACCESS_KEY_JSON | jq -r .AccessKey.SecretAccessKey)\n</code></pre> <ul> <li>Check that calls are now being executed by the <code>inlets-automation</code> IAM User.</li> </ul> <pre><code>aws sts get-caller-identity\n</code></pre> <ul> <li>Ask STS for some temporary credentials</li> </ul> <pre><code>TEMP_CREDS=$(aws sts get-session-token)\n</code></pre> <ul> <li>Break out the required elements</li> </ul> <pre><code>echo $TEMP_CREDS | jq -r .Credentials.AccessKeyId &gt; access-key.txt    echo $TEMP_CREDS | jq -r .Credentials.SecretAccessKey &gt; secret-key.txt\necho $TEMP_CREDS | jq -r .Credentials.SessionToken &gt; session-token.txt\n</code></pre> <ul> <li>Create an exit-server using temporary credentials:</li> </ul> <pre><code>inletsctl create \\\n--provider ec2 \\\n--region eu-west-1 \\\n--access-token-file ./access-key.txt \\\n--secret-key-file ./secret-key.txt \\\n--session-token-file ./session-token.txt\n</code></pre> <ul> <li>Delete an exit-server using temporary credentials:</li> </ul> <pre><code>export INSTANCEID=\"\"\n\ninletsctl delete \\\n--provider ec2 \\\n--id $INSTANCEID\n--access-token-file ./access-key.txt \\\n--secret-key-file ./secret-key.txt \\\n--session-token-file ./session-token.txt\n</code></pre>"},{"location":"reference/inletsctl/#example-usage-with-google-compute-engine","title":"Example usage with Google Compute Engine","text":"<ul> <li>One time setup required for a service account key</li> </ul> <p>It is assumed that you have gcloud installed and configured on your machine. If not, then follow the instructions here</p> <pre><code># Get current projectID\nexport PROJECTID=$(gcloud config get-value core/project 2&gt;/dev/null)\n\n# Create a service account\ngcloud iam service-accounts create inlets \\\n--description \"inlets-operator service account\" \\\n--display-name \"inlets\"\n\n# Get service account email\nexport SERVICEACCOUNT=$(gcloud iam service-accounts list | grep inlets | awk '{print $2}')\n\n# Assign appropriate roles to inlets service account\ngcloud projects add-iam-policy-binding $PROJECTID \\\n--member serviceAccount:$SERVICEACCOUNT \\\n--role roles/compute.admin\n\ngcloud projects add-iam-policy-binding $PROJECTID \\\n--member serviceAccount:$SERVICEACCOUNT \\\n--role roles/iam.serviceAccountUser\n\n# Create inlets service account key file\ngcloud iam service-accounts keys create key.json \\\n--iam-account $SERVICEACCOUNT\n</code></pre> <ul> <li>Create a tunnel using the service account and project ID</li> </ul> <pre><code># Create a TCP tunnel server\ninletsctl create \\\n--provider gce \\\n--project-id=$PROJECTID \\\n--access-token-file=key.json \\\n--tcp\n\n# Create a HTTP / HTTPS tunnel server\ninletsctl create \\\n-p gce \\\n--project-id=$PROJECTID \\\n-f=key.json\n\n# Or specify any valid Google Cloud Zone optional zone, by default it get provisioned in us-central1-a\ninletsctl create -p gce \\\n--project-id=$PROJECTID \\\n-f key.json \\\n--zone=us-central1-a\n</code></pre>"},{"location":"reference/inletsctl/#example-usage-with-azure","title":"Example usage with Azure","text":"<p>Prerequisites:</p> <ul> <li>You will need <code>az</code>. See Install the Azure CLI</li> </ul> <p>Generate Azure auth file  <pre><code>SUBSCRIPTION_ID=\"YOUR_SUBSCRIPTION_ID\"\naz ad sp create-for-rbac --role Contributor --scopes \"/subscriptions/$SUBSCRIPTION_ID\" --sdk-auth \\\n&gt; $HOME/Downloads/client_credentials.json\n</code></pre></p> <p>List Azure available regions <pre><code>az account list-locations -o table\n</code></pre></p> <p>Create <pre><code>inletsctl create --provider=azure --subscription-id=4d68ee0c-7079-48d2-b15c-f294f9b11a9e \\\n--region=eastus --access-token-file=~/Downloads/client_credentials.json </code></pre></p> <p>Delete <pre><code>inletsctl delete --provider=azure --id inlets-clever-volhard8 \\\n--subscription-id=4d68ee0c-7079-48d2-b15c-f294f9b11a9e \\\n--region=eastus --access-token-file=~/Downloads/client_credentials.json\n</code></pre></p>"},{"location":"reference/inletsctl/#example-usage-with-hetzner","title":"Example usage with Hetzner","text":"<pre><code># Obtain the API token from Hetzner Cloud Console.\nexport TOKEN=\"\"\n\ninletsctl create --provider hetzner \\\n--access-token $TOKEN \\\n--region hel1\n</code></pre> <p>Available regions are <code>hel1</code> (Helsinki), <code>nur1</code> (Nuremberg), <code>fsn1</code> (Falkenstein).</p>"},{"location":"reference/inletsctl/#example-usage-with-linode","title":"Example usage with Linode","text":"<p>Prerequisites:</p> <ul> <li>Prepare a Linode API Access Token. See Create Linode API Access token </li> </ul> <p>Create <pre><code>inletsctl create --provider=linode --access-token=&lt;API Access Token&gt; --region=us-east\n</code></pre></p> <p>Delete <pre><code>inletsctl delete --provider=linode --access-token=&lt;API Access Token&gt; --id &lt;instance id&gt;\n</code></pre></p>"},{"location":"reference/inletsctl/#example-usage-with-scaleway","title":"Example usage with Scaleway","text":"<pre><code># Obtain from your Scaleway dashboard:\nexport TOKEN=\"\"\nexport SECRET_KEY=\"\"\nexport ORG_ID=\"\"\n\ninletsctl create --provider scaleway \\\n--access-token $TOKEN\n--secret-key $SECRET_KEY --organisation-id $ORG_ID\n</code></pre> <p>The region is hard-coded to France / Paris 1.</p>"},{"location":"reference/inletsctl/#example-usage-with-ovhcloud","title":"Example usage with OVHcloud","text":"<p>You need to create API keys for the ovhCloud country/continent you're going to deploy with inletsctl.  For an overview of available endpoint check supported-apis documentation</p> <p>For, example, Europe visit https://eu.api.ovh.com/createToken to create your API keys.</p> <p>However, the specific value for the endpoint flag are following:</p> <ul> <li><code>ovh-eu</code> for OVH Europe API</li> <li><code>ovh-us</code> for OVH US API</li> <li><code>ovh-ca</code> for OVH Canada API</li> <li><code>soyoustart-eu</code> for So you Start Europe API</li> <li><code>soyoustart-ca</code> for So you Start Canada API</li> <li><code>kimsufi-eu</code> for Kimsufi Europe API</li> <li><code>kimsufi-ca</code> for Kimsufi Canada API</li> </ul> <p><code>ovh-eu</code> is the default endpoint and <code>DE1</code> the default region. </p> <p>For the proper <code>rights</code> choose all HTTP Verbs (GET,PUT,DELETE, POST), and we need only the <code>/cloud/</code> API.</p> <pre><code>export APPLICATION_KEY=\"\"\nexport APPLICATION_SECRET=\"\"\nexport CONSUMER_KEY=\"\"\nexport ENDPOINT=\"\"\nexport PROJECT_ID=\"\"\n\ninletsctl create --provider ovh \\\n--access-token $APPLICATION_KEY \\\n--secret-key $APPLICATION_SECRET --consumer-key $CONSUMER_KEY \\ \n--project-id $SERVICENAME \\\n--endpoint $ENDPOINT\n</code></pre>"},{"location":"reference/inletsctl/#the-delete-command","title":"The <code>delete</code> command","text":"<p>The delete command takes an id or IP address which are given to you at the end of the <code>inletsctl create</code> command. You'll also need to specify your cloud access token.</p> <pre><code>inletsctl delete \\\n--provider digitalocean \\\n--access-token-file ~/Downloads/do-access-token \\\n--id 164857028 \\\n</code></pre> <p>Or delete via IP:</p> <pre><code>inletsctl delete \\\n--provider digitalocean \\\n--access-token-file ~/Downloads/do-access-token \\\n--ip 209.97.131.180 \\\n</code></pre>"},{"location":"reference/inletsctl/#kfwd-kubernetes-service-forwarding","title":"kfwd - Kubernetes service forwarding","text":"<p>kfwd runs an inlets-pro server on your local computer, then deploys an inlets client in your Kubernetes cluster using a Pod. This enables your local computer to access services from within the cluster as if they were running on your laptop.</p> <p>inlets Pro allows you to access any TCP service within the cluster, using an encrypted link:</p> <p>Forward the <code>figlet</code> pod from <code>openfaas-fn</code> on port <code>8080</code>:</p> <pre><code>inletsctl kfwd \\\n--pro \\\n--license $(cat ~/LICENSE)\n--from figlet:8080 \\\n--namespace openfaas-fn \\\n--if 192.168.0.14\n</code></pre> <p>Note the <code>if</code> parameter is the IP address of your local computer, this must be reachable from the Kubernetes cluster.</p> <p>Then access the service via <code>http://127.0.0.1:8080</code>.</p>"},{"location":"reference/inletsctl/#troubleshooting","title":"Troubleshooting","text":"<p>inletsctl provisions a host called an exit node or exit server using public cloud APIs. It then  prints out a connection string.</p> <p>Are you unable to connect your client to the exit server?</p>"},{"location":"reference/inletsctl/#troubleshooting-inlets-pro","title":"Troubleshooting inlets Pro","text":"<p>If using auto-tls (the default), check that port 8123 is accessible. It should be serving a file with a self-signed certificate, run the following:</p> <pre><code>export IP=192.168.0.1\ncurl -k https://$IP:8123/.well-known/ca.crt\n</code></pre> <p>If you see connection refused, log in to the host over SSH and check the service via systemctl:</p> <pre><code>sudo systemctl status inlets-pro\n\n# Check its logs\nsudo journalctl -u inlets-pro\n</code></pre> <p>You can also check the configuration in <code>/etc/default/inlets-pro</code>, to make sure that an IP address and token are configured.</p>"},{"location":"reference/inletsctl/#configuration-using-environment-variables","title":"Configuration using environment variables","text":"<p>You may want to set an environment variable that points at your <code>access-token-file</code> or <code>secret-key-file</code></p> <p>Inlets will look for the following:</p> <pre><code># For providers that use --access-token-file\nINLETS_ACCESS_TOKEN\n\n# For providers that use --secret-key-file\nINLETS_SECRET_KEY\n</code></pre> <p>With the correct one of these set you wont need to add the flag on every command execution. </p> <p>You can set the following syntax in your <code>bashrc</code> (or equivalent for your shell)</p> <pre><code>export INLETS_ACCESS_TOKEN=$(cat my-token.txt)\n\n# or set the INLETS_SECRET_KEY for those providors that use this\nexport INLETS_SECRET_KEY=$(cat my-token.txt)\n</code></pre>"},{"location":"tutorial/automated-http-server/","title":"Automated http server","text":""},{"location":"tutorial/automated-http-server/#automate-a-http-tunnel-server","title":"Automate a HTTP tunnel server","text":"<p>Learn how to serve traffic from your private network over a private tunnel server.</p> <p>At the end of this tutorial, you'll have a a secure TLS public endpoint using your own DNS and domain, which you can use to access your internal services or webpages.</p> <p>I'll show you how to:</p> <ul> <li>automate a tunnel server on a public cloud provider with inlets pre-loaded onto it, </li> <li>how to connect a client from your home or private network</li> <li>how to tunnel one or more services</li> <li>and what else you can do </li> </ul> <p>In a previous article, I explained some of the differences between SaaS and private tunnel servers.</p>"},{"location":"tutorial/automated-http-server/#create-your-tunnel-server","title":"Create your tunnel server","text":"<p>With SaaS tunnels, your tunnels server processes run on shared servers with other users. With a private tunnel server like inlets, you need to create a server somewhere on the Internet to run the tunnel. It should be created with a public IP address that you can use to accept traffic and proxy it into your private network.</p> <p></p> <p>Pictured: Inlets Conceptual architecture</p> <p>The simplest way to do this is to use the inletsctl tool, which supports around a dozen clouds. The alternative is to set up a VPS or install inlets-pro onto a server you already have set up, and then add a systemd unit file so that it restarts if the tunnel or server should crash for any reason.</p> <p>To see a list of supported clouds run:</p> <pre><code>inletsctl create --help\n</code></pre> <p>For instructions on how to create an API key or service account for each, feel free to browse the docs.</p> <pre><code>inletsctl create \\\n--region lon1 \\\n--provider digitalocean \\\n--access-token-file ~/digital-ocean-api-key.txt \\\n--letsencrypt-domain blog.example.com \\\n--letsencrypt-email webmaster@example.com\n</code></pre> <p>A VM will be created in your account using the cheapest plan available, for DigitalOcean this costs 5 USD / mo at time of writing.</p> <p>You can also run your tunnel server in the free tier of GCP, Oracle Cloud or on Fly.io at no additional cost.</p> <p>Once the tunnel server has been created, you will receive:</p> <ul> <li>The IP address</li> <li>An endpoint for the inlets client to connect to</li> <li>A token for the inlets client to use when connecting</li> </ul> <p>Take a note of these.</p> <p>Now create a DNS \"A\" record for the IP address of the tunnel server on your domain control panel.</p> <p>Personally, I'm a fan of Google Domains and the .dev domains, but DigitalOcean can also manage domains through their CLI:</p> <pre><code>export IP=\"\"\nexport SUBDOMAIN=\"blog.example.com\"\n\ndoctl compute domain create $SUBDOMAIN \\\n--ip-address $IP\n</code></pre> <p>How does the TLS encryption work?</p> <p>The inlets server process will attempt to get a TLS certificate from Let's Encrypt using a HTTP01 Acme challenge.</p> <p>What if I have multiple sites?</p> <p>You can pass a number of sub-domains, for instance:</p> <pre><code> --letsencrypt-domain blog.example.com,grafana.example.com \\\n--letsencrypt-email webmaster@example.com\n</code></pre>"},{"location":"tutorial/automated-http-server/#connect-your-tunnel-client","title":"Connect your tunnel client","text":"<p>The tunnel client can be run as and when required, or you can generate a systemd unit file so that you can have it running in the background. You can run the tunnel on the same machine as the service that you're proxying, or you can run it on another computer. It's entirely up to you.</p> <p>So you could have a Raspberry Pi which just runs Raspberry Pi OS Lite and an inlets client, and nothing else. In this way you're creating a kind of router appliance.</p> <p>Let's imagine you've run a Node.js express service on your computer:</p> <pre><code>$ git clone https://github.com/alexellis/alexellis.io \\\n--depth=1\n$ cd alexellis.io/\n$ npm install\n$ npm start\n\nalexellis.io started on port: http://0.0.0.0:3000\n</code></pre> <p>inlets also has its own built-in file-server with password protection and the ability to disable browsing for sharing private links. You can expose the built-in file-server when you want to share files directly, without having to upload them first: The simple way to share files directly from your computer</p> <p>You can download the inlets client using the inletsctl tool:</p> <pre><code>$ sudo inletsctl download\n</code></pre> <p>Now you can start the tunnel client and start serving a test version of my personal homepage <code>alexellis.io</code>:</p> <pre><code>$ export URL=\"\"\n$ export TOKEN=\"\"\n\n$ inlets-pro http client \\\n--url $URL \\\n--token $TOKEN \\\n--upstream blog.example.com=http://127.0.0.1:3000\n</code></pre> <p>What if my services are running on different computers?</p> <p>If they are all within the same network, then you can run the client in one place and have it point at the various internal IP addresses.</p> <pre><code>$ inlets-pro http client \\\n--url $URL \\\n--token $TOKEN \\\n--upstream blog.example.com=http://127.0.0.1:3000 \\\n--upstream grafana.example.com=http://192.168.0.100:3000\n</code></pre> <p>If they are on different networks, you can simply run multiple clients, just change the <code>--upstream</code> flag on each client.</p> <p>How can I run the client in the background?</p> <p>For Linux hosts, you can generate a systemd unit file for inlets by using the <code>--generate systemd</code> flag to the client or server command.</p> <p>Then simply copy the resulting file to the correct location on your system and install it:</p> <pre><code>$ export URL=\"\"\n$ export TOKEN=\"\"\n\n$ inlets-pro http client \\\n--url $URL \\\n--token $TOKEN \\\n--upstream blog.example.com=http://127.0.0.1:3000 \\\n--generate=systemd &gt; inlets.service\n\n$ sudo cp inlets.service /etc/systemd/system/\n$ sudo systemctl enable inlets\n</code></pre> <p>You can then check the logs or service status:</p> <pre><code>$ sudo journalctl -u inlets\n$ sudo systemctl status inlets\n</code></pre>"},{"location":"tutorial/automated-http-server/#access-your-website-over-the-tunnel","title":"Access your website over the tunnel","text":"<p>You can now access your local website being served at http://127.0.0.1:3000 over the tunnel by visiting the domain you created:</p> <p>https://blog.example.com/</p>"},{"location":"tutorial/automated-http-server/#your-ip-goes-where-you-go","title":"Your IP goes where you go","text":"<p>You can close the lid on your laptop, and open it again in Starbucks or your favourite local independent coffee shop. As soon as you reconnect the client, your local server will be available over the tunnel at the same IP address and domain: https://blog.example.com/</p> <p>I used this technique to test a live demo for the KubeCon conference. I then took a flight from London to San Diego and was able to receive traffic to my Raspberry Pi whilst tethering on a local SIM card.</p> <p></p> <p>Tethering my Raspberry Pi with K3s in San Diego</p>"},{"location":"tutorial/automated-http-server/#wrapping-up","title":"Wrapping up","text":"<p>In a very short period of time we created a private tunnel server on a public cloud of our choice, then we created a DNS record for it, and connected a client and accessed our local website.</p> <p>You can get started with inlets through a monthly subscription, or save on a yearly plan.</p> <p>When would you need this?</p> <ul> <li>If you're self-hosting websites, you already have some equipment at home, so it can work out cheaper.</li> <li>If you're running a Kubernetes cluster or K3s on a Raspberry Pi, it can be much cheaper over the course of a year.</li> <li>But it's also incredibly convenient for sharing files and for testing APIs or OAuth flows during development.</li> </ul> <p>Ben Potter at Coder is writing up a tutorial on how to access a private VSCode server from anywhere using a private tunnel. If you would like to learn more, follow @inletsdev for when it gets published.</p> <p></p> <p>Andrew Meier put it this way:</p> <p>\"I prefer to play around with different projects without having to worry about my costs skyrocketing. I had a few Raspberry Pis and wondered if I could use them as a cluster. After a bit of searching #k3s and  inlets gave me my answer\"</p> <p></p> <p>Andrew's K3s cluster, with inlets</p> <p>Read his blog post: Personal Infrastructure with Inlets, k3s, and Pulumi</p>"},{"location":"tutorial/automated-http-server/#you-may-also-like","title":"You may also like","text":"<ul> <li>Tunnel a service or ingress from Kubernetes</li> <li>Share a file without uploading it through inlets tunnels</li> <li>Connecting my boat to the Internet with inlets</li> </ul>"},{"location":"tutorial/caddy-http-tunnel/","title":"Caddy http tunnel","text":""},{"location":"tutorial/caddy-http-tunnel/#custom-reverse-proxy-with-caddy","title":"Custom reverse proxy with Caddy","text":"<p>In this tutorial we'll set up an inlets TCP tunnel server to forward ports 80 and 443 to a reverse proxy server running on our local machine. Caddy will receive a TCP stream from the public tunnel server for ports 80 and 443. It can terminate TLS and also allow you to host multiple sites with ease.</p> <p>Caddy is a free and open-source reverse proxy. It's often used on web-servers to add TLS to one or more virtual websites.</p>"},{"location":"tutorial/caddy-http-tunnel/#pre-reqs","title":"Pre-reqs","text":"<ul> <li>A Linux server, Windows and MacOS are also supported</li> <li>The inlets-pro binary at /usr/local/bin/</li> <li>Access to a DNS control plane for a domain you control</li> </ul> <p>You can run through the same instructions with other reverse proxies such as Nginx, or Traefik.</p> <p>Scenario: * You want to share a file such as a VM image or a ISO over the Internet, with HTTPS, directly from your laptop. * You have one or more websites or APIs running on-premises or within your home-lab and want to expose them on the Internet.</p> <p>You can subscribe to inlets for personal or commercial use via Gumroad</p>"},{"location":"tutorial/caddy-http-tunnel/#setup-your-exit-node","title":"Setup your exit node","text":"<p>Provision a cloud VM on DigitalOcean or another IaaS provider using inletsctl:</p> <pre><code>inletsctl create \\\n--provider digitalocean \\\n--region lon1 \\\n--pro\n</code></pre> <p>Note the <code>--url</code> and <code>TOKEN</code> given to you in this step.</p>"},{"location":"tutorial/caddy-http-tunnel/#setup-your-dns-a-record","title":"Setup your DNS A record","text":"<p>Setup a DNS A record for the site you want to expose using the public IP of the cloud VM</p> <ul> <li><code>178.128.40.109</code> = <code>service.example.com</code></li> </ul>"},{"location":"tutorial/caddy-http-tunnel/#run-a-local-server-to-share-files","title":"Run a local server to share files","text":"<p>Do not run this command in your home folder.</p> <pre><code>mkdir -p /tmp/shared/\ncd /tmp/shared/\n\necho \"Hello world\" &gt; WELCOME.txt\n\n# If using Python 2.x\npython -m SimpleHTTPServer\n\n# Python 3.x\npython3 -m http.server\n</code></pre> <p>This will listen on port <code>8000</code> by default.</p>"},{"location":"tutorial/caddy-http-tunnel/#setup-caddy-1x","title":"Setup Caddy 1.x","text":"<ul> <li>Download the latest Caddy 1.x binary from the Releases page</li> </ul> <p>Pick your operating system, for instance Darwin for MacOS, or Linux.</p> <p>Download the binary, extract it and install it to <code>/usr/local/bin</code>:</p> <pre><code>mkdir -p /tmp/caddy\ncurl -sLSf https://github.com/caddyserver/caddy/releases/download/v1.0.4/caddy_v1.0.4_darwin_amd64.zip &gt; caddy.tar.gz\ntar -xvf caddy.tar.gz --strip-components=0 -C /tmp/caddy\n\nsudo cp /tmp/caddy/caddy /usr/local/bin/\n</code></pre> <ul> <li>Create a Caddyfile</li> </ul> <p>The <code>Caddyfile</code> configures which websites Caddy will expose, and which sites need a TLS certificate.</p> <p>Replace <code>service.example.com</code> with your own domain.</p> <p>Next, edit <code>proxy / 127.0.0.1:8000</code> and change the port <code>8000</code> to the port of your local webserver, for instance <code>3000</code> or <code>8080</code>. For our example, keep it as <code>8000</code>.</p> <pre><code>service.example.com\n\nproxy / 127.0.0.1:8000 {\ntransparent\n}\n</code></pre> <p>Start the Caddy binary, it will listen on port 80 and 443.</p> <pre><code>sudo ./caddy\n</code></pre> <p>If you have more than one website, you can add them to the Caddyfile on new lines.</p> <p>You'll need to run caddy as <code>sudo</code> so that it can bind to ports 80, and 443 which require additional privileges.</p>"},{"location":"tutorial/caddy-http-tunnel/#start-the-inlets-pro-client-on-your-local-side","title":"Start the inlets-pro client on your local side","text":"<p>Downloads the inlets Pro client:</p> <pre><code>sudo inletsctl download\n</code></pre> <p>Run the inlets-pro client, using the TOKEN and IP given to you from the previous step.</p> <p>The client will look for your license in <code>$HOME/.inlets/LICENSE</code>, but you can also use the <code>--license/--license-file</code> flag if you wish.</p> <pre><code>export IP=\"\"        # take this from the exit-server\nexport TOKEN=\"\"     # take this from the exit-server\n\ninlets-pro tcp client \\\n--url wss://$IP:8123/connect \\\n--ports 80,443 \\\n--token $TOKEN \\\n--upstream localhost\n</code></pre> <p>Note that <code>--upstream localhost</code> will connect to Caddy running on your computer, if you are running Caddy on another machine, use its IP address here.</p>"},{"location":"tutorial/caddy-http-tunnel/#check-it-all-worked","title":"Check it all worked","text":"<p>You'll see that Caddy can now obtain a TLS certificate.</p> <p>Go ahead and visit: <code>https://service.example.com</code></p> <p>Congratulations, you've now served a TLS certificate directly from your laptop. You can close caddy and open it again at a later date. Caddy will re-use the certificate it already obtained and it will be valid for 3 months. To renew, just keep Caddy running or open it again whenever you need it.</p>"},{"location":"tutorial/caddy-http-tunnel/#setup-caddy-2x","title":"Setup Caddy 2.x","text":"<p>For Caddy 2.x, the Caddyfile format changes.</p> <p>Let's say you're running a Node.js service on port 3000, and want to expose it with TLS on the domain \"service.example.com\":</p> <pre><code>git clone https://github.com/alexellis/expressjs-k8s/\ncd expressjs-k8s\n\nnpm install\nhttp_port=3000 npm start\n</code></pre> <p>The local site will be served at http://127.0.0.1:3000</p> <pre><code>{\n  acme_ca https://acme-staging-v02.api.letsencrypt.org/directory\n}\n\nservice.example.com\n\nreverse_proxy 127.0.0.1:3000 {\n}\n</code></pre> <p>Note the <code>acme_ca</code> being used will receive a staging certificate, remove it to obtain a production TLS certificate.</p> <p>Now download Caddy 2.x for your operating system.</p> <pre><code>sudo ./caddy run \\\n-config ./Caddyfile\n</code></pre> <p><code>sudo</code> - is required to bind to port 80 and 443, although you can potentially update your OS to allow binding to low ports without root access.</p> <p>You should now be able to access the Node.js website via the <code>https://service.example.com</code> URL.</p> <p>Caddy also supports multiple domains within the same file, so that you can expose multiple internal or private websites through the same tunnel.</p> <pre><code>{\n  email \"webmaster@example.com\"\n}\n\nblog.example.com {\n  reverse_proxy 127.0.0.1:4000\n}\n\nopenfaas.example.com {\n      reverse_proxy 127.0.0.1:8080\n}\n</code></pre> <p>If you have services running on other machines you can change <code>127.0.0.1:8080</code> to a different IP address such as that of your Raspberry Pi if you had something like OpenFaaS running there.</p>"},{"location":"tutorial/community/","title":"Community tutorials and guides","text":"<p>Note: Any material not hosted on <code>inlets.dev</code> may be written by a third-party.</p> <p>If you have a tutorial or video to submit, feel free to send a Pull Request</p>"},{"location":"tutorial/community/#case-studies","title":"Case studies","text":"<p>You can read testimonials on the main homepage</p> <ul> <li>Connecting my boat to the Internet with inlets by Mark Sharpley</li> <li>How Riskfuel is using Inlets to build machine learning models at scale by Addison van den Hoeven</li> <li>Ingress to ECS Anywhere, from anywhere, using Inlets by Nathan Peck</li> <li>Reliable local port-forwarding from Kubernetes for a Developer at UK Gov</li> </ul>"},{"location":"tutorial/community/#videos","title":"Videos","text":"<p>Webinars:</p> <ul> <li>A tale of two networks - demos and use-cases for inlets tunnels (Mar 2021) by Alex Ellis and Johan Siebens</li> <li>Crossing network boundaries with Kubernetes and inlets (Mar 2021) by Alex Ellis and Johan Siebens</li> </ul> <p>Walk-through videos:</p> <ul> <li>inlets-operator - Get Ingress and Public IPs for private Kubernetes (Mar 2020) by Alex Ellis</li> <li>Inlets Operator - get a LoadBalancer from any Kubernetes cluster (Oct 2019) by Alex Ellis</li> <li>Hacking on the Inlets Operator for Equinix Metal (Jul 2021) by Alex Ellis and David McKay</li> </ul>"},{"location":"tutorial/community/#tutorials","title":"Tutorials","text":"<ul> <li>A Tour of Inlets - A Tunnel Built for the Cloud (Aug 2021) by Zespre Schmidt</li> <li>Control Access to your on-prem services with Cloud IAP and inlets Pro (Dec 2020) by Johan Siebens</li> <li>Secure access using HashiCorp Boundary &amp; inlets Pro Better Together (Oct 2020) by Johan Siebens</li> <li>Quake III Arena, k3s and a Raspberry Pi (Nov 2020) by Johan Siebens</li> <li>Argo CD for your private Raspberry Pi k3s cluster (Aug 2020) by Johan Siebens</li> <li>Get a TLS-enabled Docker registry in 5 minutes (Feb 2020) by Alex Ellis</li> <li>A bit of Istio before tea-time (May 2021) by Alex Ellis</li> <li>Get kubectl access to your private cluster from anywhere (Jan 2020) by Alex Ellis</li> <li>Exploring Kubernetes Operator Pattern (Jan 2021) by Ivan Velichko</li> </ul>"},{"location":"tutorial/community/#official-blog-posts","title":"Official blog posts","text":"<p>See inlets.dev/blog</p>"},{"location":"tutorial/dual-tunnels/","title":"Dual tunnels","text":""},{"location":"tutorial/dual-tunnels/#setting-up-dual-tcp-and-https-tunnels","title":"Setting up dual TCP and HTTPS tunnels","text":"<p>In this tutorial we will set both a dual tunnel for exposing HTTP and TCP services from the same server. </p> <p>Whilst it's easier to automate two separate servers or cloud instances for your tunnels, you may want to reduce your costs.</p> <p>The use-case may be that you have a number of OpenFaaS functions running on your Raspberry Pi which serve traffic to users, but you also want to connect via SSH and VNC.</p>"},{"location":"tutorial/dual-tunnels/#pre-reqs","title":"Pre-reqs","text":"<ul> <li>A Linux server, Windows and MacOS are also supported</li> <li>The inlets-pro binary at /usr/local/bin/</li> <li>Access to a DNS control plane for a domain you control</li> </ul>"},{"location":"tutorial/dual-tunnels/#create-the-https-tunnel-server-first","title":"Create the HTTPS tunnel server first","text":"<p>Create a HTTPS tunnel server using the manual tutorial or automated tutorial.</p> <p>Once it's running, check you can connect to it, and then log in with SSH.</p> <p>You'll find a systemd service named <code>inlets-pro</code> running the HTTPS tunnel with a specific authentication token and set of parameters.</p> <p>Now, generate a new systemd unit file for the TCP tunnel.</p> <p>I would suggest generating a new token for this tunnel.</p> <pre><code>TOKEN=\"$(head -c 32 /dev/urandom | base64 | cut -d \"-\" -f1)\"\n\n# Find the instance's public IPv4 address:\nPUBLIC_IP=\"$(curl -s https://checkip.amazonaws.com)\"\n</code></pre> <p>Let's imagine the public IP resolved to <code>46.101.128.5</code> which is part of the DigitalOcean range.</p> <pre><code>inlets-pro tcp server \\\n--token \"$TOKEN\" \\\n--auto-tls-san $PUBLIC_IP \\\n--generate=systemd &gt; inlets-pro-tcp.service\n</code></pre> <p>Example:</p> <pre><code>[Unit]\nDescription=inlets Pro TCP Server\nAfter=network.target\n\n[Service]\nType=simple\nRestart=always\nRestartSec=5\nStartLimitInterval=0\nExecStart=/usr/local/bin/inlets-pro tcp server --auto-tls --auto-tls-san=46.101.128.5 --control-addr=0.0.0.0 --token=\"k1wCR+2j41TXqqq/UTLJzcuzhmSJbU5NY32VqnNOnog=\" --control-port=8124 --auto-tls-path=/tmp/inlets-pro-tcp\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>We need to update the control-port for this inlets tunnel server via the <code>--control-port</code> flag. Use port 8124 since 8123 is already in use by the HTTP tunnel. Add <code>--control-port 8124</code> to the <code>ExecStart</code> line.</p> <p>We need to add a new flag so that generated TLS certificates are placed in a unique directory, and don't clash. Add <code>--auto-tls-path /tmp/inlets-pro-tcp/</code> to the same line.</p> <p>Next install the unit file with:</p> <pre><code>sudo cp inlets-pro-tcp.service /etc/systemd/system/\nsudo systemctl daemon-reload\nsudo systemctl enable inlets-pro-tcp.service\n\nsudo systemctl restart inlets-pro-tcp.service\n</code></pre> <p>You'll now be able to check the logs for the server:</p> <pre><code>sudo journalctl -u inlets-pro-tcp\n</code></pre> <p>Finally you can connect your TCP client:</p> <pre><code>inlets-pro tcp client \\\n--token \"k1wCR+2j41TXqqq/UTLJzcuzhmSJbU5NY32VqnNOnog=\" \\\n--upstream 192.168.0.15 \\\n--ports 2222,5900 \\\n--url wss://46.101.128.5:8124\n</code></pre> <p>Note that <code>5900</code> is the default port for VNC. Port <code>2222</code> was used for SSH as not to conflict with the version running on the tunnel server.</p> <p>You can now connect to the public IP of your server via SSH and VNC:</p> <p>For example:</p> <pre><code>ssh -p 2222 pi@46.101.128.5\n</code></pre>"},{"location":"tutorial/dual-tunnels/#wrapping-up","title":"Wrapping up","text":"<p>You now have a TCP and HTTPS tunnel server running on the same host. This was made possibly by changing the control-plane port and auto-TLS path for the second server, and having it start automatically through a separate systemd service.</p> <p>This technique may save you a few dollars per month, but it may not be worth your time compared to how quick and easy it is to create two separate servers with <code>inletsctl create</code>.</p>"},{"location":"tutorial/istio-gateway/","title":"Tutorial: Expose an Istio gateway with the inlets-operator","text":"<p>In this tutorial we will configure the inlets-operator to get a public IP for the Istio Ingress Gateway. This will allow you to receive HTTPS certificates via LetsEncrypt and cert-manager and access services running in your cluster on their own public domain.</p>"},{"location":"tutorial/istio-gateway/#install-arkade","title":"Install arkade","text":"<p>Arkade is a simple CLI tool that provides a quick way to install various apps and download common binaries much quicker.</p> <p>To install arkade run:</p> <pre><code>curl -sSLf https://get.arkade.dev/ | sudo sh\n</code></pre>"},{"location":"tutorial/istio-gateway/#create-a-kubernetes-cluster-with-kind","title":"Create a kubernetes cluster with kinD","text":"<p>We're going to use KinD, which runs inside a container with Docker for Mac or the Docker daemon. MacOS cannot actually run containers or Kubernetes itself, so projects like Docker for Mac create a small Linux VM and hide it away.</p> <p>Download the kind and kubectl binaries if you don't have them already:</p> <pre><code>arkade get kind\narkade get kubectl\n</code></pre> <p>Now create a cluster:</p> <pre><code>$ kind create cluster\n</code></pre> <p>The initial creation could take a few minutes, but subsequent clusters creations are much faster.</p> <pre><code>Creating cluster \"kind\" ...\n \u2713 Ensuring node image (kindest/node:v1.19.0) \ud83d\uddbc\n \u2713 Preparing nodes \ud83d\udce6  \n \u2713 Writing configuration \ud83d\udcdc \n \u2713 Starting control-plane \ud83d\udd79\ufe0f \n \u2713 Installing CNI \ud83d\udd0c \n \u2713 Installing StorageClass \ud83d\udcbe \nSet kubectl context to \"kind-kind\"\nYou can now use your cluster with:\n\nkubectl cluster-info --context kind-kind\n\nHave a nice day! \ud83d\udc4b\n</code></pre> <pre><code>kubectl get node -o wide\n\nNAME                 STATUS     ROLES    AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE       KERNEL-VERSION     CONTAINER-RUNTIME\nkind-control-plane      Ready   master   35s   v1.18.0   172.17.0.2    &lt;none&gt;        Ubuntu 19.10   5.3.0-26-generic   containerd://1.3.2\n</code></pre> <p>The above shows one node is Ready, so we can move on and install Istio.</p>"},{"location":"tutorial/istio-gateway/#install-istio","title":"Install Istio","text":"<p>You can install Istio using the documentation site at Istio.io, but we're going to use arkade instead since it gives us a one-line install and also bundles a version of Istio configuration for constrained development environments like a KinD cluster.</p> <p>It is always possible to use the <code>--set</code> flag to override or pass in additional values for the Istio chart.</p> <pre><code>arkade install istio --help\n\nInstall istio\n\nUsage:\n  arkade install istio [flags]\n\nExamples:\n  arkade install istio --loadbalancer\n\nFlags:\n      --cpu string               Allocate CPU resource (default \"100m\")\n  -h, --help                     help for istio\n      --istio-namespace string   Namespace for the app (default \"istio-system\")\n      --memory string            Allocate Memory resource (default \"100Mi\")\n      --namespace string         Namespace for the app (default \"default\")\n      --profile string           Set istio profile (default \"default\")\n      --set stringArray          Use custom flags or override existing flags \n                                 (example --set prometheus.enabled=false)\n  -v, --version string           Specify a version of Istio (default \"1.11.4\")\n\nGlobal Flags:\n      --kubeconfig string   Local path for your kubeconfig file\n      --wait                If we should wait for the resource to be ready before returning (helm3 only, default false)\n</code></pre> <p>Install Istio:</p> <pre><code>arkade install istio\n</code></pre> <p>At the moment we don't have a public IP for the Istio gateway. The next step is te install the inlets operator so we can get one.</p> <pre><code>kubectl get -n istio-system \\\n  svc/istio-ingressgateway\n\nNAME                   TYPE           CLUSTER-IP     EXTERNAL-IP   PORT(S)                                      AGE\nistio-ingressgateway   LoadBalancer   10.43.92.145   &lt;pending&gt;     15021:32382/TCP,80:31487/TCP,443:31692/TCP   3m28s\n</code></pre>"},{"location":"tutorial/istio-gateway/#install-the-inlets-operator","title":"Install the inlets-operator","text":"<p>The inlets-operator lets you get public LoadBalancers on your local Kubernetes cluster. It does this by creating a VM to run an inlets tunnel server in the cloud of your choice for each LoadBalancer. It then plumbs in an inlets client to connect to it using a deployment.</p> <p>The inlets-operator can also be installed with arkade.</p> <p>Save an access token for your cloud provider as <code>$HOME/access-token</code>, in this example we're using DigitalOcean. Other providers may also need a secret token in addition to the API key.</p> <p>Your inlets license should be already saved at: <code>$HOME/.inlets/LICENSE</code>, if it's not, you can move it there or use the <code>--license-file</code> flag.</p> <pre><code>export ACCESS_TOKEN=$HOME/access-token\n\narkade install inlets-operator \\\n--provider digitalocean \\\n--region lon1 \\\n--token-file $ACCESS_TOKEN \\\n--license-file \"$HOME/.inlets/LICENSE\"\n</code></pre> <p>You can run <code>arkade install inlets-operator --help</code> to see a list of other cloud providers or take a look at the inlets-operator reference documentation.</p> <ul> <li>Set the <code>--region</code> flag as required, it's best to have low latency between your current location and where the exit-servers will be provisioned.</li> </ul> <p>Once the inlets-operator is installed we can start watching for the public IP to appear.</p> <pre><code>kubectl get -n istio-system \\\nsvc/istio-ingressgateway -w\n\nNAME                   TYPE           CLUSTER-IP     EXTERNAL-IP\nistio-ingressgateway   LoadBalancer   10.106.220.170   &lt;pending&gt;\nistio-ingressgateway   LoadBalancer   10.106.220.170   165.227.237.77\n</code></pre>"},{"location":"tutorial/istio-gateway/#install-cert-manager","title":"Install cert-manager","text":"<p>Install cert-manager, which can be integrated with Istio gateways to manage TLS certificates.</p> <pre><code>arkade install cert-manager\n</code></pre>"},{"location":"tutorial/istio-gateway/#a-quick-recap","title":"A quick recap","text":"<p>This is what we have so far:</p> <ul> <li> <p>Istio</p> <p>The istio service mesh. Among other things, it comes with the istio Ingress Gateway that will get a public address via an inlets tunnel.</p> </li> <li> <p>inlets-operator</p> <p>The inlets operator provides us with a public VirtualIP for the istio Ingress Gateway</p> </li> <li> <p>cert-manager</p> <p>Integrates with Istio gateways to provide TLS certificates through the HTTP01 or DNS01 challenges from LetsEncrypt.</p> </li> </ul>"},{"location":"tutorial/istio-gateway/#deploy-an-application-and-get-a-tls-certificate","title":"Deploy an application and get a TLS certificate","text":"<p>Istio uses the Bookinfo Application as an example in their documentation. We will also use this example.</p> <p>Enable side-car injection and then deploy the BookInfo manifests:</p> <pre><code>kubectl label namespace default istio-injection=enabled\n\nkubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.15/samples/bookinfo/platform/kube/bookinfo.yaml\n\nkubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.15/samples/bookinfo/networking/bookinfo-gateway.yaml\n</code></pre> <p>We can verify that the book application is up and running and accessible form our local computer on local host by running: <pre><code>kubectl port-forward -n istio-system \\\n  svc/istio-ingressgateway 31380:80\n</code></pre></p> <p>Then send a request to it with <code>curl</code>:</p> <pre><code>curl -sS http://127.0.0.1:31380/productpage | grep -o \"&lt;title&gt;.*&lt;/title&gt;\"\n&lt;title&gt;Simple Bookstore App&lt;/title&gt;\n</code></pre> <p>Since we set up the inlets operator in the previous step to get an external IP for the Istio ingress gateway we should now also be able to access the app using that public IP.</p> <p>Open a browser and navigate to the /productpage URL using the EXTERNAL-IP:</p> <pre><code>http://165.227.237.77/productpage\n</code></pre> <p></p> <p>TLS certificates require a domain name and DNS A or CNAME entry. You can create those in the admin panel of your provider. They should point to the external IP of the Istio Ingress gateway. We will use the <code>bookinfo.example.com</code> domain as an example.</p> <pre><code>export EMAIL=\"you@example.com\"\n\ncat &gt; issuer-prod.yaml &lt;&lt;EOF\napiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\n  name: letsencrypt-prod\nspec:\n  acme:\n    server: https://acme-v02.api.letsencrypt.org/directory\n    email: $EMAIL\n    privateKeySecretRef:\n      name: letsencrypt-prod\n    solvers:\n    - selector: {}\n      http01:\n        ingress:\n          class: istio\nEOF\n</code></pre> <p>Note that ingress class is set to <code>class: istio</code>.</p> <p>We are using the Let's Encrypt production server which has strict limits on the API. A staging server is also available at <code>https://acme-staging-v02.api.letsencrypt.org/directory</code>. If you are creating a lot of certificates while testing a deployment it would be better to use the staging server.</p> <p>Edit <code>email</code>, then run: <code>kubectl apply -f issuer-prod.yaml</code>.</p> <p>Create a new certificate resource <pre><code>apiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\nname: ingress-cert\nnamespace: istio-system\nspec:\nsecretName: ingress-cert\ncommonName: bookinfo.example.com\ndnsNames:\n- bookinfo.example.com\nissuerRef:\nname: letsencrypt-prod\nkind: ClusterIssuer\n</code></pre></p> <p>Edit the bookinfo gateway, <code>kubectl edit gateway/bookinfo-gateway</code> and reference the certificate secret in the TLS configuration under <code>credentialName</code>.</p> <pre><code>apiVersion: networking.istio.io/v1beta1\nkind: Gateway\nmetadata:\nname: bookinfo-gateway\nspec:\nselector:\nistio: ingressgateway # use istio default controller\nservers:\n- port:\nnumber: 443\nname: https\nprotocol: HTTPS\ntls:\nmode: SIMPLE\ncredentialName: ingress-cert # This should match the Certificate secretName\nhosts:\n- bookinfo.example.com\n</code></pre> <p>You can always checkout the Istio documentation for more information on how to integrate cert-manager.</p> <p>We can use curl again to access the bookinfo application this time with our custom domain and over a secure connection. Alternatively you can open the URL in your browser.</p> <pre><code>curl -sS https://bookinfo.example.com/productpage | grep -o \"&lt;title&gt;.*&lt;/title&gt;\"\n&lt;title&gt;Simple Bookstore App&lt;/title&gt;\n</code></pre>"},{"location":"tutorial/istio-gateway/#wrapping-up","title":"Wrapping up","text":"<p>Through the use of the inlets-operator we were able to get a public IP for the Istio Ingress gateway. This allows you to access services on your cluster whether you are running it in an on-premises datacenter, within a VM or on your local laptop.</p> <p>There is no need to open a firewall port, set-up port-forwarding rules, configure dynamic DNS or any of the usual hacks. You will get a public IP and it will \"just work\" for any TCP traffic you may have. </p>"},{"location":"tutorial/kubernetes-api-server/","title":"Tutorial: Expose a local Kubernetes API Server","text":"<p>In this tutorial, we'll show you how to expose a local Kubernetes API Server on the Internet, so that you can access it from anywhere, just like with a managed cloud provider.</p>"},{"location":"tutorial/kubernetes-api-server/#pre-reqs","title":"Pre-reqs","text":"<ul> <li>A computer or laptop running MacOS or Linux, or Git Bash or WSL on Windows</li> <li>Docker for Mac / Docker Daemon - installed in the normal way, you probably have this already</li> <li>Kubernetes running locally with kubeadm, K3s, K3d, Minikube, KinD, Docker Desktop, etc</li> </ul>"},{"location":"tutorial/kubernetes-api-server/#the-kubernetes-cluster","title":"The Kubernetes cluster","text":"<p>By default every Kubernetes cluster has TLS enabled to encrypt any HTTP REST messages that go over its control-plane. The TLS certificate has to be bound to a certain name, sometimes called a TLS SAN.</p> <p>The certificate is usually only valid for \"kubernetes.default.svc\", and can only be accessed from within the cluster.</p> <p></p> <p>Kubernetes on tour - get access to your cluster from anywhere, without having to resort to complex tooling like VPNs.</p> <p>When a managed cloud provider provisions you a cluster, they'll add additional names into the certificate like \"customer1.lke.eu.linode.com\" which is then added to your generated kubeconfig file that you download in the dashboard.</p> <p>We have five steps run through to expose the API server:</p> <p>1) Create a Kubernetes cluster 2) Create a VM on the public cloud with an inlets TCP server running on it 3) Create a DNS entry for the public VM's IP address 4) Configure a TLS SAN, if possible with a new domain name 5) Set up an inlets client as a Pod to forward traffic to the Kubernetes API Server</p> <p>Once we have all this in place, we can take our existing kubeconfig file and edit the URL, so that instead of pointing at our LAN IP or localhost, it points to the domain mapped to the public VM.</p>"},{"location":"tutorial/kubernetes-api-server/#create-a-cluster","title":"Create a cluster","text":"<p>You can create a cluster on any machine by using KinD:</p> <pre><code>arkade get kind\nkind create cluster\n</code></pre> <p>If you have a Raspberry Pi or a Linux Server, you can install K3s using <code>k3sup</code>:</p> <pre><code>arkade get k3sup\n\nk3sup install --ip 192.168.1.101 --user pi\n</code></pre> <p>In either case, you'll get back a kubeconfig file.</p> <p>Here's a snippet of what I got back from running <code>k3sup install</code>:</p> <pre><code>apiVersion: v1\nclusters:\n- cluster:\nserver: https://192.168.1.101:6443\n</code></pre> <p>The server field will need to be changed to the new public address later on.</p>"},{"location":"tutorial/kubernetes-api-server/#create-a-vm-on-the-public-cloud-with-an-inlets-tcp-server-running-on-it","title":"Create a VM on the public cloud with an inlets TCP server running on it","text":"<p>Just like when Linode Kubernetes Engine provisions us a domain like <code>\"customer1.lke.eu.linode.com\"</code>, we'll need our own subdomain too, so that the certificate can be issued for it.</p> <p>In order to create the DNS record, we a public IP which we will get by creating a tunnel server on our preferred cloud and in a region that's close to us.</p> <pre><code>arkade get inletsctl\n\nexport ACCESS_TOKEN=\"\" # Retreive this from your cloud dashboard\n\ninletsctl create \\\n--provider linode \\\n--tcp \\\n--access-token $ACCESS_TOKEN \\\n--region eu-west\n</code></pre> <p>Save the connection info from inletsctl into a text file for later.</p> <pre><code># Give a single value or comma-separated\nexport PORTS=\"8000\"\n\n# Where to route traffic from the inlets server\nexport UPSTREAM=\"localhost\"\n\ninlets-pro tcp client --url \"wss://139.160.201.143:8123\" \\\n  --token \"f2cXtOouRpuVbAn4arVvdSMx//uKD3jDnssr3X9P338\" \\\n  --upstream $UPSTREAM \\\n  --ports $PORTS\n</code></pre> <p>Create a DNS subdomain for the IP address you were given:</p> <ul> <li><code>k3s.example.com</code> =&gt; <code>139.160.201.143</code></li> </ul> <p>Check that you can resolve the IP with a ping <code>ping -c 1 k3s.example.com</code></p> <p>Now check the status of the inlets server:</p> <pre><code>export TOKEN=\"f2cXtOouRpuVbAn4arVvdSMx//uKD3jDnssr3X9P338\"\n\ninlets-pro status --url \"wss://139.160.201.143:8123\" \\\n--token \"$TOKEN\"\n</code></pre> <p>Output:</p> <pre><code>inlets server status. Version: 0.9.3 - 8e96997499ae53c6fb2ae9f9e13fa9b48dcb6514\n\nServer info:\nHostname:       localhost\nProcess uptime: 5 seconds ago\nMode:           tcp\nVersion:        0.9.3 8e96997499ae53c6fb2ae9f9e13fa9b48dcb6514\n\nNo clients connected.\n</code></pre> <p>We can now move onto the next step.</p>"},{"location":"tutorial/kubernetes-api-server/#configure-a-tls-san-if-possible-with-a-new-domain-name","title":"Configure a TLS SAN, if possible with a new domain name","text":"<p>With k3s, it's trivial to add additional TLS SAN names for the Kubernetes API Server.</p> <p>If you run the <code>k3sup install</code> command again, it'll update your configuration:</p> <pre><code>k3sup install \\\n--ip 192.168.1.101 \\\n--user pi \\\n--tls-san k3s.example.com\n</code></pre> <p>You'll now have the custom domain along with the default <code>kubernetes.default.svc</code> as valid names in the generated certificate.</p> <p>If you're not running on k3s, or use a service where you cannot change the TLS SAN, then we'll show you what to do in the next step.</p>"},{"location":"tutorial/kubernetes-api-server/#update-your-kubeconfig-file-with-the-new-endpoint","title":"Update your kubeconfig file with the new endpoint","text":"<p>We need to update our kubeconfig file to point at the custom domain instead of at whatever loopback or LAN address it currently does.</p> <p>For K3s users, change the server URL:</p> <pre><code>apiVersion: v1\nclusters:\n- cluster:\nserver: https://192.168.1.101:6443\n</code></pre> <p>To:</p> <pre><code>apiVersion: v1\nclusters:\n- cluster:\nserver: https://k3s.example.com:443\n</code></pre> <p>For any user where you cannot regenerate the TLS certificate for the API Server, you can specify the server name in the config file:</p> <pre><code>apiVersion: v1\nclusters:\n- cluster:\nserver: https://k3s.example.com:443\ntls-server-name: kubernetes\n</code></pre> <p>For more details see: Support TLS Server Name overrides in kubeconfig file #88769</p> <p>Save the changes to your kubeconfig file.</p>"},{"location":"tutorial/kubernetes-api-server/#connect-the-tunnel","title":"Connect the tunnel","text":"<p>The tunnel acts like a router, it takes any TCP packets sent to port 6443 (k3s) or 443 (Kubernetes) and forwards them down the tunnel to the inlets client. The inlets client then looks at its own \"--upstream\" value to decide where to finally send the data.</p> <p>Save <code>inlets-k8s-api.yaml</code>:</p> <pre><code>export LICENSE=\"$(cat $HOME/.inlets/LICENSE)\"\nexport TOKEN=\"f2cXtOouRpuVbAn4arVvdSMx//uKD3jDnssr3X9P338\" # populate with the token from inletsctl\nexport SERVER_IP=\"139.160.201.143\" # populate with the server IP, not the domain\n\ncat &gt; inlets-k8s-api.yaml &lt;&lt;EOF\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: inlets-client\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: inlets-client\n  template:\n    metadata:\n      labels:\n        app: inlets-client\n    spec:\n      containers:\n      - name: inlets-client\n        image: ghcr.io/inlets/inlets-pro:0.9.9\n        imagePullPolicy: IfNotPresent\n        command: [\"inlets-pro\"]\n        args:\n        - \"tcp\"\n        - \"client\"\n        - \"--url=wss://$SERVER_IP:8123\"\n        - \"--upstream=kubernetes.default.svc\"\n        - \"--port=443\"\n        - \"--port=6443\"\n        - \"--token=$TOKEN\"\n        - \"--license=$LICENSE\"\n---\nEOF\n</code></pre> <p>You'll see the tunnel client up and running and ready to receive requests:</p> <pre><code>kubectl logs deploy/inlets-client\n2022/06/24 09:51:18 Licensed to: Alex &lt;contact@openfaas.com&gt;, expires: 128 day(s)\n2022/06/24 09:51:18 Upstream server: kubernetes.default.svc, for ports: 443, 6443\ntime=\"2022/06/24 09:51:18\" level=info msg=\"Connecting to proxy\" url=\"wss://139.160.201.143:8123/connect\"\ninlets-pro TCP client. Copyright OpenFaaS Ltd 2021\ntime=\"2022/06/24 09:51:18\" level=info msg=\"Connection established\" client_id=5309466072564c1c90ce0a0bcaa22b74\n</code></pre> <p>Check the tunnel server's status to confirm the connection:</p> <pre><code>export TOKEN=\"f2cXtOouRpuVbAn4arVvdSMx//uKD3jDnssr3X9P338\"\n\ninlets-pro status --url \"wss://139.160.201.143:8123\" \\\n--token \"$TOKEN\"\n\ninlets server status. Version: 0.9.3 - 8e96997499ae53c6fb2ae9f9e13fa9b48dcb6514\n\nServer info:\nHostname:       localhost\nProcess uptime: 15 minutes ago\nMode:           tcp\nVersion:        0.9.3 8e96997499ae53c6fb2ae9f9e13fa9b48dcb6514\n\nConnected clients:\nClient ID                        Remote Address        Connected  Upstreams\n5309466072564c1c90ce0a0bcaa22b74 192.168.1.101:16368 43 seconds kubernetes.default.svc:443, kubernetes.default.svc:6443\n</code></pre> <p>Finally prove that it's working with the new, public address:</p> <pre><code>$ kubectl cluster-info\nKubernetes control plane is running at https://k3s.example.com:443\nCoreDNS is running at https://k3s.example.com:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\nMetrics-server is running at https://k3s.example.com:443/api/v1/namespaces/kube-system/services/https:metrics-server:https/proxy\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n</code></pre>"},{"location":"tutorial/kubernetes-api-server/#wrapping-up","title":"Wrapping up","text":"<p>In a relatively short period of time, with a custom domain, and a small VM, we set up a tunnel server to route traffic from the public Internet to a K3s server on an internal network.</p> <p>This gives you a similar experience to a managed public cloud Kubernetes engine, but running on your own infrastructure, or perhaps within a restrictive VPC.</p> <p>You may also like:</p> <ul> <li>Learn how to manage apps across multiple Kubernetes clusters by Johan Siebens</li> </ul> <p>If you'd like to talk to us about this tutorial, feel free to reach out for a meeting:</p> <p>Set up a meeting</p>"},{"location":"tutorial/kubernetes-ingress/","title":"Tutorial: Expose a local IngressController with the inlets-operator","text":"<p>In this quick-start we will configure the inlets-operator to use inlets-pro in TCP mode to expose ports 80 and 443 of an Ingress Controller (ingress-nginx) so that it can receive HTTPS certificates via LetsEncrypt and cert-manager.</p> <p>The inlets-operator creates a VM for each tunnel server in the cloud of your choice, then plumbs in an inlets client to connect to it using a Deployment. There is an alternative approach that we also recommend which involves creating the tunnel server with inletsctl, followed by installing the inlets client with Helm: Fixing Ingress for short-lived local Kubernetes clusters.</p> <p>You can subscribe to inlets for personal or commercial use via Gumroad</p>"},{"location":"tutorial/kubernetes-ingress/#pre-reqs","title":"Pre-reqs","text":"<ul> <li>A computer or laptop running MacOS or Linux, or Git Bash or WSL on Windows</li> <li>Docker for Mac / Docker Daemon - installed in the normal way, you probably have this already</li> <li>KinD - the \"darling\" of the Kubernetes community is Kubernetes IN Docker, a small one-shot cluster that can run inside a Docker container</li> <li>arkade - arkade is an app installer that takes a helm chart and bundles it behind a simple CLI</li> </ul>"},{"location":"tutorial/kubernetes-ingress/#install-arkade","title":"Install arkade","text":"<p>You can use arkade or helm to install the various applications we are going to add to the cluster below. arkade provides an apps ecosystem that makes things much quicker.</p> <p>MacOS and Linux users:</p> <pre><code>curl -sSLf https://get.arkade.dev/ | sudo sh\n</code></pre> <p>Windows users should install Git Bash and run the above without <code>sudo</code>.</p>"},{"location":"tutorial/kubernetes-ingress/#create-a-kubernetes-cluster-with-kind","title":"Create a Kubernetes cluster with KinD","text":"<p>We're going to use KinD, which runs inside a container with Docker for Mac or the Docker daemon. MacOS cannot actually run containers or Kubernetes itself, so projects like Docker for Mac create a small Linux VM and hide it away.</p> <p>You can use an alternative to KinD if you have a preferred tool.</p> <p>Get a KinD binary release and <code>kubectl</code> (the Kubernetes CLI):</p> <pre><code>arkade get kind --version v0.9.0\narkade get kubectl --version v1.19.3\n</code></pre> <p>Now create a cluster:</p> <pre><code>$ kind create cluster\n</code></pre> <p>The initial creation could take a few minutes, but subsequent clusters creations are much faster.</p> <pre><code>Creating cluster \"kind\" ...\n \u2713 Ensuring node image (kindest/node:v1.19.0) \ud83d\uddbc\n \u2713 Preparing nodes \ud83d\udce6  \n \u2713 Writing configuration \ud83d\udcdc \n \u2713 Starting control-plane \ud83d\udd79\ufe0f \n \u2713 Installing CNI \ud83d\udd0c \n \u2713 Installing StorageClass \ud83d\udcbe \nSet kubectl context to \"kind-kind\"\nYou can now use your cluster with:\n\nkubectl cluster-info --context kind-kind\n\nHave a nice day! \ud83d\udc4b\n</code></pre> <p>We can check that our single node is ready now:</p> <pre><code>kubectl get node -o wide\n\nNAME                 STATUS     ROLES    AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE       KERNEL-VERSION     CONTAINER-RUNTIME\nkind-control-plane      Ready   master   35s   v1.18.0   172.17.0.2    &lt;none&gt;        Ubuntu 19.10   5.3.0-26-generic   containerd://1.3.2\n</code></pre> <p>The above shows one node Ready, so we are ready to move on.</p>"},{"location":"tutorial/kubernetes-ingress/#install-the-inlets-operator","title":"Install the inlets-operator","text":"<p>Save an access token for your cloud provider as <code>$HOME/access-token</code>, in this example we're using DigitalOcean. Other providers may also need a secret token in addition to the API key.</p> <p>Your inlets license should be already saved at: <code>$HOME/.inlets/LICENSE</code>, if it's not, you can move it there or use the <code>--license-file</code> flag.</p> <pre><code>export ACCESS_TOKEN=$HOME/access-token\n\narkade install inlets-operator \\\n--provider digitalocean \\\n--region lon1 \\\n--token-file $ACCESS_TOKEN \\\n--license-file \"$HOME/.inlets/LICENSE\"\n</code></pre> <p>You can run <code>arkade install inlets-operator --help</code> to see a list of other cloud providers.</p> <ul> <li>Set the <code>--region</code> flag as required, it's best to have low latency between your current location and where the exit-servers will be provisioned.</li> </ul>"},{"location":"tutorial/kubernetes-ingress/#install-nginx-ingress","title":"Install nginx-ingress","text":"<p>This installs nginx-ingress using its Helm chart:</p> <pre><code>arkade install nginx-ingress\n</code></pre>"},{"location":"tutorial/kubernetes-ingress/#install-cert-manager","title":"Install cert-manager","text":"<p>Install cert-manager, which can obtain TLS certificates through NginxIngress.</p> <pre><code>arkade install cert-manager\n</code></pre>"},{"location":"tutorial/kubernetes-ingress/#a-quick-review","title":"A quick review","text":"<p>Here's what we have so far:</p> <ul> <li> <p>nginx-ingress</p> <p>An IngressController, Traefik or Caddy are also valid options. It comes with a Service\u00a0of type LoadBalancer that will get a public address via the tunnel</p> </li> <li> <p>inlets-operator configured to use inlets-pro in TCP mode</p> <p>Provides us with a public VirtualIP for the IngressController service.</p> </li> <li> <p>cert-manager</p> <p>Provides TLS certificates through the HTTP01 or DNS01 challenges from LetsEncrypt</p> </li> </ul>"},{"location":"tutorial/kubernetes-ingress/#deploy-an-application-and-get-a-tls-certificate","title":"Deploy an application and get a TLS certificate","text":"<p>This is the final step that shows everything working end to end.</p> <p>TLS certificates require a domain name and DNS A or CNAME entry, so let's set that up</p> <p>Find the External-IP:</p> <pre><code>kubectl get svc\n</code></pre> <p>Now create a DNS A record in your admin panel, so for example: <code>expressjs.example.com</code>.</p> <p>Now when you install a Kubernetes application with an Ingress definition, NginxIngress and cert-manager will work together to provide a TLS certificate.</p> <p>Create a staging issuer for cert-manager <code>staging-issuer.yaml</code> and make sure you edit the <code>email</code> value.</p> <pre><code>export EMAIL=\"you@example.com\"\n\ncat &gt; issuer-staging.yaml &lt;&lt;EOF\napiVersion: cert-manager.io/v1\nkind: Issuer\nmetadata:\n  name: letsencrypt-staging\n  namespace: default\nspec:\n  acme:\n    server: https://acme-staging-v02.api.letsencrypt.org/directory\n    email: $EMAIL\n    privateKeySecretRef:\n      name: letsencrypt-staging\n    solvers:\n    - selector: {}\n      http01:\n        ingress:\n          class: nginx\nEOF\n</code></pre> <p>Apply the file with <code>kubectl apply -f staging-issuer.yaml</code></p> <p>While the Let's Encrypt production server has strict limits on the API, the staging server is more forgiving, and should be used while you are testing a deployment.</p> <p>Edit <code>email</code>, then run: <code>kubectl apply -f issuer.yaml</code>.</p> <p>Let's use helm3 to install Alex's example Node.js API available on GitHub</p> <p>Create a <code>custom.yaml</code> file with the following:</p> <pre><code>ingress:\nenabled: true\nannotations:\nkubernetes.io/ingress.class: nginx\ncert-manager.io/issuer: \"letsencrypt-staging\"\nhosts:\n- host: expressjs.inlets.dev\npaths: [\"/\"]\ntls:\n- secretName: expressjs-tls\nhosts:\n- expressjs.inlets.dev\n</code></pre> <p>Replace the string <code>expressjs.inlets.dev</code> with your own sub-domain created earlier i.e. <code>expressjs.example.com</code>.</p> <p>You can download around a dozen other CLI tools using arkade including helm. Use arkade to download helm and put it in your <code>PATH</code>:</p> <pre><code>arkade get helm\n\n# Put arkade in your path:\nexport PATH=$PATH:$HOME/.arkade/bin/helm3/\n\n# Or alternatively install to /usr/local/bin\nsudo cp $HOME/.arkade/bin/helm3/helm /usr/local/bin/\n</code></pre> <p>Now install the chart using helm:</p> <pre><code>helm repo add expressjs-k8s https://alexellis.github.io/expressjs-k8s/\n\n# Then they run an update\nhelm repo update\n\n# And finally they install\nhelm upgrade --install express expressjs-k8s/expressjs-k8s \\\n--values custom.yaml\n</code></pre>"},{"location":"tutorial/kubernetes-ingress/#test-it-out","title":"Test it out","text":"<p>Now check the certificate has been created and visit the webpage in a browser:</p> <pre><code>kubectl get certificate\n\nNAME            READY   SECRET          AGE\nexpressjs-tls   True    expressjs-tls   49s\n</code></pre> <p>Open the webpage i.e. https://api.example.com. Since this is a staging certificate, you will get a warning from your browser. You can accept the certificate in order to test your site.</p>"},{"location":"tutorial/kubernetes-ingress/#getting-a-production-certificate","title":"Getting a Production Certificate","text":"<p>Create a production certificate issuer <code>issuer-prod.yaml</code>, similar to the staging issuer you produced earlier. Be sure to change the email address to your email.</p> <pre><code>export EMAIL=\"you@example.com\"\n\ncat &gt; issuer-prod.yaml &lt;&lt;EOF\napiVersion: cert-manager.io/v1\nkind: Issuer\nmetadata:\n  name: letsencrypt-prod\n  namespace: default\nspec:\n  acme:\n    server: https://acme-v02.api.letsencrypt.org/directory\n    email: $EMAIL\n    privateKeySecretRef:\n      name: letsencrypt-prod\n    solvers:\n    - selector: {}\n      http01:\n        ingress:\n          class: nginx\nEOF\n</code></pre> <p>Then run <code>kubectl apply -f issuer-prod.yaml</code></p> <p>Now you must update your <code>expressjs</code> deployment to use the new certificate issuer. Create a new helm3 overrides file <code>custom-prod.yaml</code>:</p> <pre><code>cat &gt; custom-prod.yaml &lt;&lt;EOF\ningress:\n  enabled: true\n  annotations:\n    kubernetes.io/ingress.class: nginx\n    cert-manager.io/issuer: \"letsencrypt-prod\"\n  hosts:\n    - host: expressjs.inlets.dev\n      paths: [\"/\"]\n  tls:\n   - secretName: expressjs-tls\n     hosts:\n       - expressjs.inlets.dev\nEOF\n</code></pre> <p>Be sure to change the above domain name to your domain name for the sample server.</p> <p>You can update your deployment using the helm command below:</p> <pre><code>helm upgrade express expressjs-k8s/expressjs-k8s \\\n--values custom-prod.yaml\n</code></pre> <p>Here's my example on my own domain:</p> <p></p> <p>You can view the certificate the certificate that's being served directly from your local cluster and see that it's valid:</p> <p></p>"},{"location":"tutorial/kubernetes-ingress/#install-a-real-world-application","title":"Install a real-world application","text":"<p>Using arkade you can now install OpenFaaS or a Docker Registry with a couple of commands, and since you have Nginx and cert-manager in place, this will only take a few moments.</p>"},{"location":"tutorial/kubernetes-ingress/#openfaas-with-tls","title":"OpenFaaS with TLS","text":"<p>OpenFaaS is a platform for Kubernetes that provides FaaS functionality and microservices. The motto of the project is Serverless Functions Made Simple and you can deploy it along with TLS in just a couple of commands:</p> <pre><code>export DOMAIN=gateway.example.com\narkade install openfaas\narkade install openfaas-ingress \\\n--email webmaster@$DOMAIN \\\n--domain $DOMAIN\n</code></pre> <p>That's it, you'll now be able to access your gateway at https://$DOMAIN/</p> <p>For more, see the OpenFaaS workshop</p>"},{"location":"tutorial/kubernetes-ingress/#docker-registry-with-tls","title":"Docker Registry with TLS","text":"<p>A self-hosted Docker Registry with TLS and private authentication can be hard to set up, but we can now do that with two commands.</p> <pre><code>export DOMAIN=registry.example.com\narkade install docker-registry\narkade install docker-registry-ingress \\\n--email webmaster@$DOMAIN \\\n--domain $DOMAIN\n</code></pre> <p>Now try your registry:</p> <pre><code>docker login $DOMAIN\ndocker pull alpine:3.16\ndocker tag alpine:3.16 $DOMAIN/alpine:3.16\n\ndocker push $DOMAIN/alpine:3.16\n</code></pre> <p>You can even combine the new private registry with OpenFaaS if you like, checkout the docs for more.</p>"},{"location":"tutorial/kubernetes-ingress/#wrapping-up","title":"Wrapping up","text":"<p>Through the use of inlets-pro we have an encrypted control-plane for the websocket tunnel, and encryption for the traffic going to our Express.js app using a TLS certificate from LetsEncrypt.</p> <p>You can now get a green lock and a valid TLS certificate for your local cluster, which also means that this will work with bare-metal Kubernetes, on-premises and with your Raspberry Pi cluster.</p>"},{"location":"tutorial/manual-http-server/","title":"Manual http server","text":""},{"location":"tutorial/manual-http-server/#setting-up-a-http-tunnel-server-manually","title":"Setting up a HTTP tunnel server manually","text":"<p>In this tutorial we will set up an inlets HTTP tunnel server to serve a local website over HTTPS using Let's Encrypt. The steps will be manual, but usually, we would use a provisioning tool like inletsctl to automate everything for us.</p> <p>This may be useful for understanding how the server binary works, and how to use it on existing servers that you may have. Or perhaps you want to run inlets across an internal or private network.</p>"},{"location":"tutorial/manual-http-server/#pre-reqs","title":"Pre-reqs","text":"<ul> <li>A Linux server, Windows and MacOS are also supported</li> <li>The inlets-pro binary at /usr/local/bin/</li> <li>Access to a DNS control plane for a domain you control</li> </ul>"},{"location":"tutorial/manual-http-server/#run-the-server","title":"Run the server","text":"<p>For this example, your tunnel server should be accessible from the Internet. The tunnel client will connect to it and then expose one or more local websites so that you can access them remotely.</p> <p>Create a DNS A record for the subdomain or subdomains you want to use, and have each of them point to the public IP address of the server you have provisioned. These short have a short TTL such as 60s to avoid waiting too long for DNS to propagate throughout the Internet. You can increase this value to a higher number later.</p> <p>First generate an authentication token that the client will use to log in:</p> <pre><code>TOKEN=\"$(head -c 32 /dev/urandom | base64 | cut -d \"-\" -f1)\"\n</code></pre> <p>We'll use the built-in support for Let's Encrypt to get a valid HTTPS certificate for any services you wish to expose via your tunnel server. It is also possible to turn off Let's Encrypt support and use your own reverse proxy such as Caddy or Nginx.</p> <pre><code>export DOMAIN=\"example.com\"\n\ninlets-pro http server \\\n--auto-tls \\\n--control-port 8123 \\\n--auto-tls-san 192.168.0.10 \\\n--letsencrypt-domain subdomain1.$DOMAIN \\\n--letsencrypt-domain subdomain2.$DOMAIN \\\n--letsencrypt-email contact@$DOMAIN \\\n--letsencrypt-issuer staging\n  --token $TOKEN\n</code></pre> <p>Notice that <code>--letsencrypt-domain</code> can be provided more than one, for each of your subdomains.</p> <p>We are also defaulting to the \"staging\" provider for TLS certificates which allows us to obtain a large number of certificates for experimentation purposes only. The default value, if this field is left off is <code>prod</code> as you will see by running <code>inlets-pro http server --help</code>.</p> <p>Now the following will happen:</p> <ul> <li>The tunnel server will start up and listen to TCP traffic on port 80 and 443.</li> <li>The server will try to resolve each of your domains passed via <code>--letsencrypt-domain</code>.</li> <li>Then once each resolves, Let's Encrypt will be contacted for a HTTP01 ACME challenge.</li> <li>Once the certificates are obtained, the server will start serving the HTTPS traffic.</li> </ul> <p>Now you can connect your client running on another machine.</p> <p>Of course you can tunnel whatever HTTP service you like, if you already have one.</p> <p>Inlets has a built-in HTTP server that we can run on our local / private machine to share files with others. Let's use that as our example:</p> <pre><code>mkdir -p /tmp/share\n\necho \"Welcome to my filesharing service.\" &gt; /tmp/share/welcome.txt\n\ninlets-pro fileserver \\\n--allow-browsing \\\n--webroot /tmp/share/\n --port 8080\n</code></pre> <p>Next let's expose that local service running on localhost:8080 via the tunnel server:</p> <pre><code>export TOKEN=\"\" # Obtain this from your server\nexport SERVER_IP=\"\" # Your server's IP\nexport DOMAIN=\"example.com\"\n\ninlets-pro http client \\\n--url wss://$SERVER_IP:8123 \\\n--token $TOKEN \\\n--upstream http://localhost:8080/\n</code></pre> <p>If you set up your server for more than one sub-domain then you can specify a domain for each local service such as:</p> <pre><code>  --upstream subdomain1.$DOMAIN=http://localhost:8080/,subdomain2.$DOMAIN=http://localhost:3000/\n</code></pre> <p>Now that your client is connected, you can access the HTTP fileserver we set up earlier via the public DNS name:</p> <pre><code>curl -k -v https://subdomain1.$DOMAIN/welcome.txt\n</code></pre> <p>Now that you can see everything working, with a staging certificate, you can run the server command again and switch out the <code>--letsencrypt-issuer staging</code> flag for <code>--letsencrypt-issuer prod</code>.</p>"},{"location":"tutorial/manual-http-server/#wrapping-up","title":"Wrapping up","text":"<p>You have now installed an inlets HTTP tunnel server to a machine by hand. The same can be achieved by running the inletsctl tool, which does all of this automatically on a number of cloud providers.</p> <ul> <li> <p>Can I connect more than one client to the same server?     Yes, and each can connect difference services. So client 1 exposes subdomain1.DOMAIN and client 2 exposes subdomain2.DOMAIN. Alternatively, you can have multiple clients exposing the same domain, for high availability.</p> </li> <li> <p>How do I keep the inlets server process running?     You can run it in the background, by using a systemd unit file. You can generate these via the <code>inlets-pro http server --generate=systemd</code> command.</p> </li> <li> <p>How do I keep the inlets client process running?     Do the same as for a server, but use the <code>inlets-pro http client --generate=systemd</code> command.</p> </li> <li> <p>What else can I do with my server?     Browse the available options for the tunnel servers with the <code>inlets-pro http server --help</code> command.</p> </li> </ul>"},{"location":"tutorial/monitoring-and-metrics/","title":"Monitoring and metrics","text":"<p>Learn how you can monitor your tunnel servers using the <code>status</code> command and Prometheus metrics.</p> <p>This can help you understand how tunnels are being used and answer questions like:</p> <ul> <li>What are the Rate, Error, Duration (RED) metrics for any HTTP APIs or websites that are being hosted?</li> <li>How many connections are open at this point in time, and on which ports?</li> <li>Have any clients attempted to connect which failed authentication?</li> </ul>"},{"location":"tutorial/monitoring-and-metrics/#introduction","title":"Introduction","text":"<p>All the information for monitoring tunnels is exposed via the inlets control-plane. It provides a connection endpoint for clients, a status endpoint and a monitoring endpoint.</p> <p>Checkout the FAQ to learn about the difference between the data-plane and control-plane</p> <p>Inlets provides two distinct ways to monitor tunnels. You can use the <code>status</code> command that is part of the CLI or collect Prometheus metrics for background monitoring and alerting. We will explore both methods.</p>"},{"location":"tutorial/monitoring-and-metrics/#the-status-command","title":"The status command","text":"<p>With the <code>inlets-pro status</code> command you can find out some basic tunnel statistics without logging in with a console SSH session. It shows you a list of the connected clients along with the version and uptime information of the server and can be used with both HTTP and TCP tunnels.</p> <p>Here\u2019s an example of a TCP tunnel server:</p> <pre><code>$ inlets-pro status \\\n--url wss://178.62.70.130:8123 \\\n--token \"$TOKEN\" \\\n--auto-tls\n\nQuerying server status. Version DEV - unknown\nHostname: unruffled-banzai4\nStarted: 49 minutes\nMode: tcp\nVersion:        0.8.9-rc1\n\nClient ID                        Remote Address     Connected Upstreams\n730aa1bb96474cbc9f7e76c135e81da8 81.99.136.188:58102 15 minutes localhost:8001, localhost:8000, localhost:2222\n22fbfe123c884e8284ee0da3680c1311 81.99.136.188:64018 6 minutes  localhost:8001, localhost:8000, localhost:2222\n</code></pre> <p>We can see the clients that are connected and the ports they make available on the server. In this case there are two clients. All traffic to the data plane for ports 8001, 8000 and 2222 will be load-balanced between the two clients for HA.</p> <p>The response from a HTTP tunnel:</p> <pre><code>$ inlets-pro status \\\n--url wss://147.62.70.101:8123 \\\n--token \"$TOKEN\"  \\\n--auto-tls\n\nServer info:\nHostname: creative-pine6\nStarted: 1 day\nMode:           http\nVersion:        0.8.9-rc1\nConnected clients:\nClient ID                        Remote Address     Connected Upstreams\n4e35edf5c6a646b79cc580984eac4ea9 192.168.0.19:34988 5 minutes example.com=http://localhost:8000, prometheus.example.com=http://localhost:9090\n</code></pre> <p>In this example we can see that there is only one client connected to the server at the moment. This client provides two separate domains.</p> <p>The command uses the status endpoint that is exposed on the control-plane. It is possible to invoke the HTTP endpoint yourself. The token that is set up for the server has to be set in the Authorization header.</p> <pre><code>$ curl -ksLS https://127.0.0.1:8123/status \\\n-H \"Authorization: Bearer $TOKEN\"\n</code></pre> <p>Example response from a HTTP tunnel:</p> <pre><code>{\n\"info\": {\n\"version\": \"0.8.9-18-gf4fc15b\",\n\"sha\": \"f4fc15b9604efd0b0ca3cc604c19c200ae6a1d7b\",\n\"mode\": \"http\",\n\"startTime\": \"2021-08-13T12:23:17.321388+01:00\",\n\"hostname\": \"am1.local\"\n},\n\"clients\": [\n{\n\"clientID\": \"0c5f2a1ca0174ee3a177c3be7cd6d950\",\n\"remoteAddr\": \"[::1]:63671\",\n\"since\": \"2021-08-13T12:23:19.72286+01:00\",\n\"upstreams\": [\n\"*=http://127.0.0.1:8080\"\n]\n}\n]\n}\n</code></pre>"},{"location":"tutorial/monitoring-and-metrics/#monitor-inlets-with-prometheus","title":"Monitor inlets with Prometheus","text":"<p>The server collects metrics for both the data-plane and the control-plane. These metrics are exposed through the monitoring endpoint on the control-plane. Prometheus can be set up for metrics collection and alerting.</p> <p>The name of the metrics and the kind of metrics that are exported will depend on the mode that the server is running in. For TCP tunnels the metric name starts with <code>tcp_</code> for HTTP tunnels this will be <code>http_</code>.</p> <p>You don\u2019t need to be a Kubernetes user to take advantage of Prometheus. You can run it locally on your machine by downloading the binary here.</p> <p>As an alternative, Grafana Cloud can give you a complete monitoring stack for your tunnels without having to worry about finding somewhere to run and maintain Prometheus and Grafana. We have a write up on our blog that shows you how to set this up: Monitor inlets tunnels with Grafana Cloud.</p> <p>Create a <code>prometheus.yaml</code> file to configure Prometheus. Replace TOKEN with the token from your server.</p> <pre><code># my global config\nglobal:\nscrape_interval:     15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.\nevaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.\n# scrape_timeout is set to the global default (10s).\n\n# Alertmanager configuration\nalerting:\nalertmanagers:\n- static_configs:\n- targets:\n# - alertmanager:9093\n\n# Load rules once and periodically evaluate them according to the global 'evaluation_interval'.\nrule_files:\n# - \"first_rules.yml\"\n# - \"second_rules.yml\"\n\n# A scrape configuration containing exactly one endpoint to scrape:\n# Here it's Prometheus itself.\nscrape_configs:\n# The job name is added as a label `job=&lt;job_name&gt;` to any timeseries scraped from this config.\n- job_name: 'prometheus'\n\n# metrics_path defaults to '/metrics'\n# scheme defaults to 'http'.\nstatic_configs:\n- targets: ['localhost:9090']\n# The job name is added as a label `job=&lt;job_name&gt;` to any timeseries scraped from this config.\n- job_name: 'http-tunnel'\n\n# metrics_path defaults to '/metrics'\n# scheme defaults to 'http'.\nstatic_configs:\n- targets: ['localhost:8123']\nscheme: https\n\nauthorization:\ntype: Bearer\ncredentials: TOKEN\ntls_config:\ninsecure_skip_verify: true\n</code></pre> <p>Start Prometheus with this command. It will listen on port 9090.</p> <pre><code>$ prometheus --config.file=./prometheus.yaml\n\nlevel=info ts=2021-08-13T11:25:31.791Z caller=main.go:428 msg=\"Starting Prometheus\" version=\"(version=2.29.1, branch=HEAD, revision=dcb07e8eac34b5ea37cd229545000b857f1c1637)\"\nlevel=info ts=2021-08-13T11:25:31.931Z caller=main.go:784 msg=\"Server is ready to receive web requests.\"\n</code></pre>"},{"location":"tutorial/monitoring-and-metrics/#metrics-for-the-control-plane","title":"Metrics for the control-plane","text":"<p>The control-plane metrics can give you insights into the number of clients that are connected and the number of http requests made to the different control-plane endpoints.</p> <p>HTTP tunnels</p> Metric Type Description Labels http_controlplane_connected_gauge gauge gauge of inlets clients connected to the control plane http_controlplane_requests_total counter total HTTP requests processed by connecting clients on the control plane <code>code</code>, <code>path</code> <p>TCP tunnels</p> Metric Type Description Labels tcp_controlplane_connected_gauge gauge gauge of inlets clients connected to the control plane tcp_controlplane_requests_total counter total HTTP requests processed by connecting clients on the control plane <code>code</code>, <code>path</code> <p>These metrics can for instance be used to tell you whether there are a lot of clients that attempted to connect but failed authentication.</p> <p>If running on Kubernetes, the connected gauge could be used to scale tunnels down to zero replicas, and back up again in a similar way to OpenFaaS. This could be important for very large-scale installations of devices or tenants that have partial connectivity.</p>"},{"location":"tutorial/monitoring-and-metrics/#metrics-for-the-data-plane","title":"Metrics for the data-plane","text":"<p>The data-plane metrics can give you insights in the services that are exposed through your tunnel.</p> <p>HTTP tunnels</p> Metric Type Description Labels http_dataplane_requests_total counter total HTTP requests processed <code>code</code>, <code>host</code>, <code>method</code> http_dataplane_request_duration_seconds histogram Seconds spent serving HTTP requests. <code>code</code>, <code>host</code>, <code>method</code> <p>TCP tunnels</p> Metric Type Description Labels tcp_dataplane_connections_gauge gauge gauge of TCP connections established over data plane <code>port</code> tcp_dataplane_connections_total counter total count of TCP connections established over data plane <code>port</code> <p>For HTTP tunnels these metrics can be used to get Rate, Error, Duration (RED) information for any API or website that is connected through the tunnel. This essentially allows you to collect basic metrics for your services even if they do not export any metrics themselves.</p> <p>For TCP tunnels these metrics can help answer questions like:</p> <ul> <li>How many connections are open at this point in time, and on which ports? i.e. if exposing SSH on port 2222, how many connections are open?</li> </ul>"},{"location":"tutorial/monitoring-and-metrics/#wrapping-up","title":"Wrapping up","text":"<p>We showed two different options that can be used to monitor your inlets tunnels.</p> <p>The CLI provides a quick and easy way to get some status information for a tunnel. The endpoint that exposes this information can also be invoked directly using HTTP.</p> <p>Prometheus metrics can be collected from the monitoring endpoint. These metrics are useful for background monitoring and alerting. They can provide you with Rate, Error, Duration (RED) metrics for HTTP services that are exposed through Inlets.</p>"},{"location":"tutorial/monitoring-and-metrics/#you-may-also-like","title":"You may also like","text":"<ul> <li>Blog post: Measure and monitor your inlets tunnels</li> </ul>"},{"location":"tutorial/postgresql-tcp-tunnel/","title":"Tutorial: Tunnel a private Postgresql database","text":"<p>In this tutorial we will tunnel Postgresql over inlets Pro to a remote machine. From there you can expose it to the Internet, or bind it to the local network for private VPN-like access.</p> <p>You can subscribe to inlets for personal or commercial use via Gumroad</p>"},{"location":"tutorial/postgresql-tcp-tunnel/#setup-your-exit-node","title":"Setup your exit node","text":"<p>Provision a cloud VM on DigitalOcean or another IaaS provider using inletsctl:</p> <pre><code>inletsctl create \\\n--provider digitalocean \\\n--region lon1 \\\n--pro\n</code></pre> <p>Note the <code>--url</code> and <code>TOKEN</code> given to you in this step.</p>"},{"location":"tutorial/postgresql-tcp-tunnel/#run-postgresql-on-your-private-server","title":"Run Postgresql on your private server","text":"<p>We can run a Postgresql instance using Docker:</p> <pre><code>head -c 16 /dev/urandom |shasum \n8cb3efe58df984d3ab89bcf4566b31b49b2b79b9\n\nexport PASSWORD=\"8cb3efe58df984d3ab89bcf4566b31b49b2b79b9\"\n\ndocker run --rm --name postgres -p 5432:5432 -e POSTGRES_PASSWORD=8cb3efe58df984d3ab89bcf4566b31b49b2b79b9 -ti postgres:latest\n</code></pre>"},{"location":"tutorial/postgresql-tcp-tunnel/#connect-the-inlets-pro-client","title":"Connect the inlets Pro client","text":"<p>Fill in the below with the outputs you received from <code>inletsctl create</code>.</p> <p>Note that <code>UPSTREAM=\"localhost\"</code> can be changed to point at a host or IP address accessible from your client. The choice of <code>localhost</code> is suitable when you are running Postgresql in Docker on the same computer as the inlets Pro client.</p> <p>The client will look for your license in <code>$HOME/.inlets/LICENSE</code>, but you can also use the <code>--license/--license-file</code> flag if you wish.</p> <pre><code>export EXIT_IP=\"134.209.21.155\"\nexport TCP_PORTS=\"5432\"\nexport LICENSE_FILE=\"$HOME/LICENSE.txt\"\nexport TOKEN=\"KXJ5Iq1Z5Cc8GjFXdXJrqNhUzoScXnZXOSRKeh8x3f6tdGq1ijdENWQ2IfzdCg4U\"\nexport UPSTREAM=\"localhost\"\n\ninlets-pro tcp client --connect \"wss://$EXIT_IP:8123/connect\" \\\n--token \"$TOKEN\" \\\n--upstream $UPSTREAM \\\n--ports $TCP_PORTS\n</code></pre>"},{"location":"tutorial/postgresql-tcp-tunnel/#connect-to-your-private-postgresql-server-from-the-internet","title":"Connect to your private Postgresql server from the Internet","text":"<p>You can run this command from anywhere, since your exit-server has a public IP:</p> <pre><code>export PASSWORD=\"8cb3efe58df984d3ab89bcf4566b31b49b2b79b9\"\nexport EXIT_IP=\"209.97.141.140\"\n\ndocker run -it -e PGPORT=5432 -e PGPASSWORD=$PASSWORD --rm postgres:latest psql -U postgres -h $EXIT_IP\n</code></pre> <p>Try a command such as <code>CREATE database</code> or <code>\\dt</code>.</p>"},{"location":"tutorial/postgresql-tcp-tunnel/#treat-the-database-as-private-like-a-vpn","title":"Treat the database as private - like a VPN","text":"<p>A split data and control-plane mean that tunnels do not need to be exposed on the Internet and can replace a VPN or a bespoke solution with SSH tunnels</p> <p>A split data and control-plane mean that tunnels do not need to be exposed on the Internet and can replace a VPN or a bespoke solution with SSH tunnels</p> <p>If you would like to keep the database service and port private, you can run the exit-server as a Pod in a Kubernetes cluster, or add an iptables rule to block access from external IPs.</p> <p>Log into your exit-server and update <code>/etc/systemd/system/inlets-pro.service</code></p> <p>To listen on loopback, add: <code>--listen-data=127.0.0.1:</code> To listen on a private adapter such as <code>10.1.0.10</code>, add: <code>--listen-data=10.1.0.10:</code></p> <p>Restart the service, and you'll now find that the database port <code>5432</code> can only be accessed from within the network you specified in <code>--listen-data</code></p> <p>Other databases such as Cassandra, MongoDB and Mysql/MariaDB also work exactly the same. Just change the port from <code>5432</code> to the port of your database.</p>"},{"location":"tutorial/ssh-tcp-tunnel/","title":"Tutorial: Expose a private SSH server over a TCP tunnel","text":"<p>In this tutorial we will use inlets-pro to access your computer behind NAT or a firewall. We'll do this by tunnelling SSH over inlets-pro, and clients will connect to your exit-server.</p> <p>Scenario: You want to allow SSH access to a computer that doesn't have a public IP, is inside a private network or behind a firewall. A common scenario is connecting to a Raspberry Pi on a home network or a home-lab.</p> <p>You can subscribe to inlets for personal or commercial use via Gumroad</p>"},{"location":"tutorial/ssh-tcp-tunnel/#setup-your-tunnel-server-with-inletsctl","title":"Setup your tunnel server with <code>inletsctl</code>","text":"<p>For this tutorial you will need to have an account and API key with one of the supported providers, or you can create an exit-server manually and install inlets Pro there yourself.</p> <p>For this tutorial, the DigitalOcean provider will be used. You can get free credits on DigitalOcean with this link.</p> <p>Create an API key in the DigitalOcean dashboard with Read and Write permissions, and download it to a file called <code>do-access-token</code> in your home directory.</p> <p>You need to know the IP of the machine you to connect to on your local network, for instance <code>192.168.0.35</code> or <code>127.0.0.1</code> if you are running inlets Pro on the same host as SSH.</p> <p>You can use the <code>inletsctl</code> utility to provision exit-servers with inlets Pro preinstalled, it can also download the <code>inlets-pro</code> CLI.</p> <pre><code>curl -sLSf https://inletsctl.inlets.dev | sh\nsudo mv inletsctl /usr/local/bin/\nsudo inletsctl download\n</code></pre> <p>If you already have <code>inletsctl</code> installed, then make sure you update it with <code>inletsctl update</code>.</p>"},{"location":"tutorial/ssh-tcp-tunnel/#create-an-tunnel-server","title":"Create an tunnel server","text":""},{"location":"tutorial/ssh-tcp-tunnel/#a-automate-your-tunnel-server","title":"A) Automate your tunnel server","text":"<p>The inletsctl tool can create a tunnel server for you in the region and cloud of your choice.</p> <pre><code>inletsctl create \\\n--provider digitalocean \\\n--access-token-file ~/do-access-token \\\n--region lon1\n</code></pre> <p>Run <code>inletsctl create --help</code> to see all the options.</p> <p>After the machine has been created, <code>inletsctl</code> will output a sample command for the <code>inlets-pro client</code> command:</p> <pre><code>inlets-pro tcp client --url \"wss://206.189.114.179:8123/connect\" \\\n--token \"4NXIRZeqsiYdbZPuFeVYLLlYTpzY7ilqSdqhA0HjDld1QjG8wgfKk04JwX4i6c6F\"\n</code></pre> <p>Don't run this command, but note down the <code>--url</code> and <code>--token</code> parameters for later</p>"},{"location":"tutorial/ssh-tcp-tunnel/#b-manual-setup-of-your-tunnel-server","title":"B) Manual setup of your tunnel server","text":"<p>Use B) if you want to provision your virtual machine manually, or if you already have a host from another provider.</p> <p>Log in to your remote tunnel server with <code>ssh</code> and obtain the binary using <code>inletsctl</code>:</p> <pre><code>curl -sLSf https://inletsctl.inlets.dev | sh\nsudo mv inletsctl /usr/local/bin/\nsudo inletsctl download\n</code></pre> <p>Find your public IP:</p> <pre><code>export IP=$(curl -s ifconfig.co)\n</code></pre> <p>Confirm the IP with <code>echo $IP</code> and save it, you need it for the client</p> <p>Get an auth token and save it for later to use with the client</p> <pre><code>export TOKEN=\"$(head -c 16 /dev/urandom |shasum|cut -d'-' -f1)\"\n\necho $TOKEN\n</code></pre> <p>Start the server:</p> <pre><code>inlets-pro \\\ntcp \\\nserver \\\n--auto-tls \\\n--auto-tls-san $IP \\\n--token $TOKEN\n</code></pre> <p>If running the inlets client on the same host as SSH, you can simply set <code>PROXY_TO_HERE</code> to <code>localhost</code>. Or if you are running SSH on a different computer to the inlets client, then you can specify a DNS entry or an IP address like <code>192.168.0.15</code>.</p> <p>If using this manual approach to install inlets Pro, you should create a systemd unit file.</p> <p>The easiest option is to run the server with the <code>--generate=systemd</code> flag, which will generate a systemd unit file to stdout. You can then copy the output to <code>/etc/systemd/system/inlets-pro.service</code> and enable it with <code>systemctl enable inlets-pro</code>.</p>"},{"location":"tutorial/ssh-tcp-tunnel/#configure-the-private-ssh-servers-listening-port","title":"Configure the private SSH server's listening port","text":"<p>It's very likely (almost certain) that your exit server will already be listening for traffic on the standard ssh port <code>22</code>. Therefore you will need to configure your internal server to use an additional TCP port such as <code>2222</code>.</p> <p>Once configured, you'll still be able to connect to the internal server on port 22, but to connect via the tunnel, you'll use port <code>2222</code></p> <p>Add the following to  <code>/etc/ssh/sshd_config</code>:</p> <pre><code>Port 22\nPort 2222\n</code></pre> <p>For (optional) additional security, you could also disable password authentication, but make sure that you have inserted your SSH key to the internal server with <code>ssh-copy-id user@ip</code> before reloading the SSH service.</p> <pre><code>PasswordAuthentication no\n</code></pre> <p>Now need to reload the service so these changes take effect</p> <pre><code>sudo systemctl daemon-reload\nsudo systemctl restart sshd\n</code></pre> <p>Check that you can still connect on the internal IP on port 22, and the new port 2222.</p> <p>Use the <code>-p</code> flag to specify the SSH port:</p> <pre><code>export IP=\"192.168.0.35\"\n\nssh -p 22 $IP \"uptime\"\nssh -p 2222 $IP \"uptime\"\n</code></pre>"},{"location":"tutorial/ssh-tcp-tunnel/#start-the-inlets-pro-client","title":"Start the inlets Pro client","text":"<p>First download the inlets-pro client onto the private SSH server:</p> <pre><code>sudo inletsctl download\n</code></pre> <p>Use the command from earlier to start the client on the server:</p> <pre><code>export IP=\"206.189.114.179\"\nexport TCP_PORTS=\"2222\"\nexport LICENSE_FILE=\"$HOME/LICENSE.txt\"\nexport UPSTREAM=\"localhost\"\n\ninlets-pro tcp client --url \"wss://$IP:8123/connect\" \\\n--token \"4NXIRZeqsiYdbZPuFeVYLLlYTpzY7ilqSdqhA0HjDld1QjG8wgfKk04JwX4i6c6F\" \\\n--license-file \"$LICENSE_FILE\" \\\n--upstream \"$UPSTREAM\" \\\n--ports $TCP_PORTS\n</code></pre> <p>The <code>localhost</code> value will be used for <code>--upstream</code> because the tunnel client is running on the same machine as the SSH service. However, you could run the client on another machine within the network, and then change the flag to point to the private SSH server's IP.</p>"},{"location":"tutorial/ssh-tcp-tunnel/#try-it-out","title":"Try it out","text":"<p>Verify the installation by trying to SSH to the public IP, using port <code>2222</code>.</p> <pre><code>ssh -p 2222 user@206.189.114.179\n</code></pre> <p>You should now have access to your server via SSH over the internet with the IP of the exit server.</p> <p>You can also use other compatible tools like <code>sftp</code>, <code>scp</code> and <code>rsync</code>, just make sure that you set the appropriate port flag. The port flag for sftp is <code>-P</code> rather than <code>-p</code>.</p>"},{"location":"tutorial/ssh-tcp-tunnel/#wrapping-up","title":"Wrapping up","text":"<p>The principles in this tutorial can be adapted for other protocols that run over TCP such as MongoDB or PostgreSQL, just adapt the port number as required.</p> <ul> <li>Quick-start: Tunnel a private database over inlets Pro</li> <li>Purchase inlets for personal or commercial use</li> </ul>"},{"location":"uplink/become-a-provider/","title":"Become an inlets uplink provider","text":"<p>inlets uplink makes it easy for Service Providers and SaaS companies to deliver their product and services to customer networks.</p> <p>To become a provider, you'll need a Kubernetes cluster, an inlets uplink subscription and to install the inlets-uplink-provider Helm chart.</p> <ul> <li>Read the Inlets Uplink announcement</li> </ul>"},{"location":"uplink/become-a-provider/#before-you-start","title":"Before you start","text":"<p>Before you start, you'll need the following:</p> <ul> <li>A Kubernetes cluster with LoadBalancer capabilities (i.e. public cloud).</li> <li>A domain name clients can use to connect to the tunnel control plane.</li> <li>An inlets uplink license (an inlets-pro license cannot be used)</li> <li> <p>Optional: arkade - a tool for installing popular Kubernetes tools</p> <p>To install arkade run:</p> <pre><code>curl -sSLf https://get.arkade.dev/ | sudo sh\n</code></pre> </li> </ul> <p>Inlets uplink has its own independent subscription from inlets-pro.</p> <p>Sign-up here: inlets uplink plans.</p>"},{"location":"uplink/become-a-provider/#create-a-kubernetes-cluster","title":"Create a Kubernetes cluster","text":"<p>We recommend creating a Kubernetes cluster with a minimum of three nodes. Each node should have a minimum of 2GB of RAM and 2 CPU cores.</p>"},{"location":"uplink/become-a-provider/#install-cert-manager","title":"Install cert-manager","text":"<p>Install cert-manager, which is used to manage TLS certificates for inlets-uplink.</p> <p>You can use Helm, or arkade:</p> <pre><code>arkade install cert-manager\n</code></pre>"},{"location":"uplink/become-a-provider/#create-a-namespace-for-the-inlets-uplink-provider-and-install-your-license","title":"Create a namespace for the inlets-uplink-provider and install your license","text":"<p>Make sure to create the target namespace for you installation first.</p> <pre><code>kubectl create namespace inlets\n</code></pre> <p>Create the required secret with your inlets-uplink license.</p> <p>Note</p> <p>There is a known issue with LemonSqueezy where the UI will copy the license key in lower-case, it needs to be converted to upper-case before being used with Inlets Uplink.</p> <p>Convert the license to upper-case, if it's in lower-case:</p> <pre><code>(\nmv $HOME/.inlets/LICENSE_UPLINK{,.lower}\n\ncat $HOME/.inlets/LICENSE_UPLINK.lower | tr '[:lower:]' '[:upper:]' &gt; $HOME/.inlets/LICENSE_UPLINK\n  rm $HOME/.inlets/LICENSE_UPLINK.lower\n)\n</code></pre> <p>Create the secret for the license:</p> <pre><code>kubectl create secret generic \\\n-n inlets inlets-uplink-license \\\n--from-file license=$HOME/.inlets/LICENSE_UPLINK\n</code></pre>"},{"location":"uplink/become-a-provider/#setup-up-ingress-for-customer-tunnels","title":"Setup up ingress for customer tunnels","text":"<p>Tunnels on your customers' network will connect to your own inlets-uplink-provider.</p> <p>There are two options for deploying the inlets-uplink-provider.</p> <p>Use Option A if you're not sure, if your team already uses Istio or prefers Istio, use Option B.</p>"},{"location":"uplink/become-a-provider/#a-install-with-kubernetes-ingress","title":"A) Install with Kubernetes Ingress","text":"<p>We recommend ingress-nginx, and have finely tuned the configuration to work well for the underlying websocket for inlets. That said, you can change the IngressController if you wish.</p> <p>Install ingress-nginx using arkade or Helm:</p> <pre><code>arkade install ingress-nginx\n</code></pre> <p>Create a <code>values.yaml</code> file for the inlets-uplink-provider chart:</p> <pre><code>clientRouter:\n# Customer tunnels will connect with a URI of:\n# wss://uplink.example.com/namespace/tunnel\ndomain: uplink.example.com\n\ntls:\nissuer:\n# Email address used for ACME registration\nemail: \"user@example.com\"\n\ningress:\nenabled: true\nclass: \"nginx\"      </code></pre> <p>Make sure to replace the domain and email with your actual domain name and email address.</p>"},{"location":"uplink/become-a-provider/#b-install-with-istio","title":"B) Install with Istio","text":"<p>We have added support in the inlets-uplink chart for Istio to make it as simple as possible to configure with a HTTP01 challenge.</p> <p>If you don't have Istio setup already you can deploy it with arkade.</p> <pre><code>arkade install istio\n</code></pre> <p>Label the <code>inlets</code> namespace so that Istio can inject its sidecars:</p> <pre><code>kubectl label namespace inlets \\\nistio-injection=enabled --overwrite\n</code></pre> <p>Create a <code>values.yaml</code> file for the inlets-uplink chart:</p> <pre><code>clientRouter:\n# Customer tunnels will connect with a URI of:\n# wss://uplink.example.com/namespace/tunnel\ndomain: uplink.example.com\n\ntls:\nissuer:\n# Email address used for ACME registration\nemail: \"user@example.com\"\n\nistio:\nenabled: true\n</code></pre> <p>Make sure to replace the domain and email with your actual domain name and email address.</p>"},{"location":"uplink/become-a-provider/#deploy-with-helm","title":"Deploy with Helm","text":"<p>The Helm chart is called inlets-uplink-provider, you can deploy it using the custom values.yaml file created above:</p> <pre><code>helm upgrade --install inlets-uplink \\\noci://ghcr.io/openfaasltd/inlets-uplink-provider \\\n--namespace inlets \\\n--values ./values.yaml\n</code></pre> <p>If you want to pin the version of the Helm chart, you can do so with the <code>--version</code> flag.</p> <p>You can browse all versions of the Helm chart on GitHub</p>"},{"location":"uplink/become-a-provider/#verify-the-installation","title":"Verify the installation","text":"<p>Once you've installed inlets-uplink, you can verify it is deployed correctly by checking the <code>inlets</code> namespace for running pods:</p> <pre><code>$ kubectl get pods --namespace inlets\n\nNAME                               READY   STATUS    RESTARTS   AGE\nclient-router-b5857cf6f-7vrdh      1/1     Running   0          92s\nprometheus-74d8d7db9b-2hptm        1/1     Running   0          16s\nuplink-operator-7fccc9bdbc-twd2q   1/1     Running   0          92s\n</code></pre> <p>You should see the <code>client-router</code> and <code>cloud-operator</code> in a <code>Running</code> state.</p> <p>If you installed inlets-uplink with Kubernetes ingress, you can verify that ingress for the client-router is setup and that a TLS certificate is issued for your domain using these two commands:</p> <pre><code>$ kubectl get -n inlets ingress/client-router\n\nNAME            CLASS    HOSTS                ADDRESS           PORTS     AGE\nclient-router   &lt;none&gt;   uplink.example.com   188.166.194.102   80, 443   31m\n</code></pre> <pre><code>$ kubectl get -n inlets cert/client-router-cert\n\nNAME                 READY   SECRET               AGE\nclient-router-cert   True    client-router-cert   30m\n</code></pre>"},{"location":"uplink/become-a-provider/#download-the-tunnel-cli","title":"Download the tunnel CLI","text":"<p>We provide a CLI to help you create and manage tunnels. It is available as a plugin for the inlets-pro CLI. </p> <p>Download the <code>inlets-pro</code> binary:</p> <ul> <li>Download it from the GitHub releases</li> <li>Get it with arkade: <code>arkade get inlets-pro</code></li> </ul> <p>Get the tunnel plugin:</p> <pre><code>inlets-pro plugin get tunnel\n</code></pre> <p>Run <code>inlets-pro tunnel --help</code> to see all available commands.</p>"},{"location":"uplink/become-a-provider/#setup-the-first-customer-tunnel","title":"Setup the first customer tunnel","text":"<p>Continue the setup here: Create a customer tunnel</p>"},{"location":"uplink/become-a-provider/#configuration-reference","title":"Configuration reference","text":"<p>Overview of inlets-uplink parameters in <code>values.yaml</code>.</p> Parameter Description Default <code>pullPolicy</code> The a imagePullPolicy applied to inlets-uplink components. <code>Always</code> <code>operator.image</code> Container image used for the uplink operator. <code>ghcr.io/openfaasltd/uplink-operator:0.1.5</code> <code>clientRouter.image</code> Container image used for the client router. <code>ghcr.io/openfaasltd/uplink-client-router:0.1.5</code> <code>clientRouter.domain</code> Domain name for inlets uplink. Customer tunnels will connect with a URI of: wss://uplink.example.com/namespace/tunnel. <code>\"\"</code> <code>clientRouter.tls.issuerName</code> Name of cert-manager Issuer for the clientRouter domain. <code>letsencrypt-prod</code> <code>clientRouter.tls.issuer.enabled</code> Create a cert-manager Issuer for the clientRouter domain. <code>true</code> <code>clientRouter.tls.issuer.email</code> Let's Encrypt email. Only used for certificate renewing notifications. <code>\"\"</code> <code>clientRouter.tls.ingress.enabled</code> Enable ingress for the client router. <code>enabled</code> <code>clientRouter.tls.ingress.class</code> Ingress class for client router ingress. <code>nginx</code> <code>clientRouter.tls.ingress.annotations</code> Annotations to be added to the client router ingress resource. <code>{}</code> <code>clientRouter.tls.istio.enabled</code> Use an Istio Gateway for incoming traffic to the client router. <code>false</code> <code>clientRouter.service.type</code> Client router service type <code>ClusterIP</code> <code>clientRouter.service.nodePort</code> Client router service port for NodePort service type, assigned automatically when left empty. (only if clientRouter.service.type is set to \"NodePort\") <code>nil</code> <code>tunnelsNamespace</code> Deployments, Services and Secrets will be created in this namespace. Leave blank for a cluster-wide scope, with tunnels in multiple namespaces. <code>\"\"</code> <code>inletsVersion</code> Inlets Pro release version for tunnel server Pods. <code>0.9.12</code> <code>clientApi.enabled</code> Enable tunnel management REST API. <code>false</code> <code>clientApi.image</code> Container image used for the client API. <code>ghcr.io/openfaasltd/uplink-api:0.1.5</code> <code>prometheus.create</code> Create the Prometheus monitoring component. <code>true</code> <code>prometheus.resources</code> Resource limits and requests for prometheus containers. <code>{}</code> <code>prometheus.image</code> Container image used for prometheus. <code>prom/prometheus:v2.40.1</code> <code>prometheus.service.type</code> Prometheus service type <code>ClusterIP</code> <code>prometheus.service.nodePort</code> Prometheus service port for NodePort service type, assigned automatically when left empty. (only if prometheus.service.type is set to \"NodePort\") <code>nil</code> <code>nodeSelector</code> Node labels for pod assignment. <code>{}</code> <code>affinity</code> Node affinity for pod assignments. <code>{}</code> <code>tolerations</code> Node tolerations for pod assignment. <code>[]</code> <p>Specify each parameter using the <code>--set key=value[,key=value]</code> argument to <code>helm install</code></p>"},{"location":"uplink/connect-to-tunnels/","title":"Connect to tunnels","text":"<p>The tunnel plugin for the inlets-pro CLI can be used to get connection instructions for a tunnel.</p> <p>Whether the client needs to be deployed as a systemd service on the customers server or as a Kubernetes service, with the CLI it is easy to generate connection instructions for these different formats by setting the <code>--format</code> flag.</p> <p>Supported formats:</p> <ul> <li>CLI command</li> <li>Systemd </li> <li>Kubernetes YAML Deployment</li> </ul> <p>Make sure you have the latest version of the tunnel command available:</p> <pre><code>inlets-pro plugin get tunnel\n</code></pre>"},{"location":"uplink/connect-to-tunnels/#get-connection-instructions","title":"Get connection instructions","text":"<p>Generate the client command for the selected tunnel:</p> <pre><code>$ inlets-pro tunnel connect openfaas \\\n--domain uplink.example.com \\\n--upstream http://127.0.0.1:8080\n\n# Access your HTTP tunnel via: http://openfaas.tunnels:8000\n\n# Access your TCP tunnel via ClusterIP: \n#  openfaas.tunnels:5432\n\ninlets-pro uplink client \\\n--url=wss://uplink.example.com/tunnels/openfaas \\\n--token=tbAd4HooCKLRicfcaB5tZvG3Qj36pjFSL3Qob6b9DBlgtslmildACjWZUD \\\n--upstream=http://127.0.0.1:8080\n</code></pre> <p>Optionally the <code>--quiet</code> flag can be set to print the CLI command without the additional info.</p>"},{"location":"uplink/connect-to-tunnels/#deploy-the-client-as-a-systemd-service","title":"Deploy the client as a systemd service","text":"<p>To generate a systemd service file for the tunnel client command set the <code>--format</code> flag to <code>systemd</code>. </p> <pre><code>$ inlets-pro tunnel connect openfaas \\\n--domain uplink.example.com \\ \n--upstream http://127.0.0.1:8080 \\\n--format systemd\n\n[Unit]\nDescription=openfaas inlets client\nAfter=network.target\n\n[Service]\nType=simple\nRestart=always\nRestartSec=5\nStartLimitInterval=0\nExecStart=/usr/local/bin/inlets-pro uplink client --url=wss://uplink.example.com/tunnels/openfaas --token=tbAd4HooCKLRicfcaB5tZvG3Qj36pjFSL3Qob6b9DBlgtslmildACjWZUD --upstream=http://127.0.0.1:8080\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>Copy the service file over to the customer's host. Save the unit file as: <code>/etc/systemd/system/openfaas-tunnel.service</code>.</p> <p>Once the file is in place start the service for the first time:</p> <pre><code>sudo systemctl daemon-reload\nsudo systemctl enable --now openfaas-tunnel\n</code></pre> <p>Verify the tunnel client is running:</p> <pre><code>systemctl status openfaas-tunnel\n</code></pre> <p>You can also check the logs to see if the client connected successfully:</p> <pre><code>journalctl -u openfaas-tunnel\n</code></pre>"},{"location":"uplink/connect-to-tunnels/#deploy-the-client-in-a-kubernetes-cluster","title":"Deploy the client in a Kubernetes cluster","text":"<p>To generate a YAML deployment for a selected tunnel, set the <code>--format</code> flag to <code>k8s_yaml</code>. The generated resource can be deployed in the customers cluster.</p> <pre><code>inlets-pro tunnel connect openfaas \\\n--domain uplink.example.com \\\n--upstream http://gateway.openfaas:8080 \\\n--format k8s_yaml\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: openfaas-inlets-client\nspec:\n  replicas: 1\nselector:\n    matchLabels:\n      app: openfaas-inlets-client\n  template:\n    metadata:\n      labels:\n        app: openfaas-inlets-client\n    spec:\n      containers:\n      - name: openfaas-inlets-client\n        image: ghcr.io/inlets/inlets-pro:0.9.14\n        imagePullPolicy: IfNotPresent\n        command: [\"inlets-pro\"]\nargs:\n        - \"uplink\"\n- \"client\"\n- \"--url=wss://uplink.example.com/tunnels/openfaas\"\n- \"--token=tbAd4HooCKLRicfcaB5tZvG3Qj36pjFSL3Qob6b9DBlgtslmildACjWZUD\"\n- \"--upstream=http://gateway.openfaas:8080\"\n</code></pre> <p>In this example we create a tunnel to uplink an OpenFaaS deployment.</p> <p>Get the logs for the client and check it connected successfully:</p> <pre><code>kubectl logs deploy/openfaas-inlets-client\n</code></pre>"},{"location":"uplink/create-tunnels/","title":"Create a tunnel for a customer","text":""},{"location":"uplink/create-tunnels/#use-separate-namespaces-for-your-tunnels","title":"Use separate namespaces for your tunnels","text":"<p>The <code>inlets</code> namespace contains the control plane for inlets uplink, so you'll need to create at least one additional namespace for your customer tunnels.</p> <ol> <li> <p>Create a namespace per customer (recommended)</p> <p>This approach avoids conflicts on names, and gives better isolation between tenants.</p> <pre><code>kubectl create namespace acmeco\n</code></pre> <p>Then, create a copy of the license secret in the new namespace:</p> <pre><code>export NS=\"n1\"\nexport LICENSE=$(kubectl get secret -n inlets inlets-uplink-license -o jsonpath='{.data.license}')\n\nkubectl create secret generic \\\n-n $NS \\\ninlets-uplink-license \\\n--from-literal license=$LICENSE\n</code></pre> </li> <li> <p>A single namespace for all customer tunnels (not recommended)</p> <p>For development purposes, you could create a single namespace for all your customers.</p> <pre><code>kubectl create namespace tunnels\n</code></pre> </li> </ol> <p>Finally, if you're using Istio, then you need to label each additional namespace to enable sidecar injection:</p> <pre><code>kubectl label namespace inlets \\\nistio-injection=enabled --overwrite\n</code></pre>"},{"location":"uplink/create-tunnels/#create-a-tunnel-with-an-auto-generated-token","title":"Create a Tunnel with an auto-generated token","text":"<p><code>Tunnel</code> describes an inlets-uplink tunnel server. The specification describes a set of ports to use for TCP tunnels.</p> <p>For example the following Tunnel configuration sets up a http tunnel on port <code>8000</code> by default and adds port <code>8080</code> for use with TCP tunnels. The <code>licenceRef</code> needs to reference a secret containing an inlets-uplink license.</p> <pre><code>apiVersion: uplink.inlets.dev/v1alpha1\nkind: Tunnel\nmetadata:\nname: acmeco\nnamespace: tunnels\nspec:\nlicenseRef:\nname: inlets-uplink-license\nnamespace: tunnels\ntcpPorts:\n- 8080 </code></pre> <p>Alternatively the CLI can be used to create a tunnel:</p> <pre><code>inlets-pro tunnel create acmeco \\\n-n tunnels\n  --port 8080\n</code></pre>"},{"location":"uplink/create-tunnels/#create-a-tunnel-with-a-pre-defined-token","title":"Create a Tunnel with a pre-defined token","text":"<p>If you delete a Tunnel with an auto-generated token, and re-create it later, the token will change. So we recommend that you pre-define your tokens. This style works well for GitOps and automated deployments with Helm.</p> <p>Make sure the secret is in the same namespace as the Tunnel Custom Resource.</p> <p>You can use <code>openssl</code> to generate a secure token:</p> <pre><code>openssl rand -base64 32 &gt; token.txt\n</code></pre> <p>Create a Kubernetes secret for the token named <code>custom-token</code>:</p> <pre><code>kubectl create secret generic \\\n-n tunnels acmeco-token \\\n--from-file token=./token.txt\n</code></pre> <p>Reference the token when creating a tunnel:</p> <pre><code>apiVersion: uplink.inlets.dev/v1alpha1\nkind: Tunnel\nmetadata:\nname: acmeco\nnamespace: tunnels\nspec:\nlicenseRef:\nname: inlets-uplink-license\nnamespace: tunnels\ntokenRef:\nname: acmeco-token\nnamespace: tunnels\ntcpPorts:\n- 8080\n</code></pre> <p>Clients can now connect to the tunnel using the custom token.</p>"},{"location":"uplink/create-tunnels/#node-selection-and-annotations-for-tunnels","title":"Node selection and annotations for tunnels","text":"<p>The tunnel spec has a <code>nodeSelector</code> field that can be used to assign tunnel pods to Nodes. See Assign Pods to Nodes from the kubernetes docs for more information.</p> <p>It is also possible to set additional annotations on the tunnel pod using the <code>podAnnotations</code> field in the tunnel spec.</p> <p>The following example adds an annotation with the customer name to the tunnel pod and uses the node selector to specify a target node with a specific region label.</p> <pre><code>apiVersion: uplink.inlets.dev/v1alpha1\nkind: Tunnel\nmetadata:\nname: acmeco\nnamespace: tunnels\nspec:\nlicenseRef:\nname: inlets-uplink-license\nnamespace: tunnels\ntcpPorts:\n- 8080\npodAnnotations:\ncutomer: acmeco\nnodeSelector:\nregion: east\n</code></pre>"},{"location":"uplink/create-tunnels/#connect-to-tunnels","title":"Connect to tunnels","text":"<p>The <code>uplink client</code> command is part of the inlets-pro binary. It is used to connect to tunnels and expose services over the tunnel.</p> <p>There are several ways to get the binary:</p> <ul> <li>Download it from the GitHub releases</li> <li>Get it with arkade: <code>arkade get inlets-pro</code></li> <li>Use the inlets-pro docker image</li> </ul>"},{"location":"uplink/create-tunnels/#example-tunnel-a-customer-http-service","title":"Example: Tunnel a customer HTTP service","text":"<p>We'll use inlets-pro's built in file server as an example of how to tunnel a HTTP service.</p> <p>Run this command on a private network or on your workstation:</p> <pre><code>mkdir -p /tmp/share\ncd /tmp/share\necho \"Hello World\" &gt; README.md\n\ninlets-pro fileserver -w /tmp/share -a\n\nStarting inlets Pro fileserver. Version: 0.9.10-rc1-1-g7bc49ae - 7bc49ae494bd9ec789fc5e9eaf500f2b1fe60786\nServing files from: /tmp/share\nListening on: 127.0.0.1:8080, allow browsing: true, auth: false\n</code></pre> <p>Once the server is running connect to your tunnel using the inlets-uplink client. We will connect to the tunnel called <code>acmeco</code> (see the example in Create a tunnel for a customer using the Custom Resource to create this tunnel).</p> <p>Retrieve the token for the tunnel:</p> kubectlcli <pre><code>kubectl get -n tunnels \\\nsecret/acmeco -o jsonpath=\"{.data.token}\" | base64 --decode &gt; token.txt </code></pre> <pre><code>inlets-pro tunnel token acmeco \\\n-n tunnels &gt; token.txt\n</code></pre> <p>The contents will be saved in token.txt.</p> <p>Start the tunnel client:</p> <pre><code>inlets-pro uplink client \\\n--url wss://uplink.example.com/tunnels/acmeco \\\n--upstream http://127.0.0.1:8080 \\\n--token-file ./token.txt\n</code></pre> <p>Tip: get connection instructions</p> <p>The tunnel plugin for the inlets-pro CLI can be used to get connection instructions for a tunnel.</p> <pre><code>inlets-pro tunnel connect acmeco \\\n--domain uplink.example.com \\\n--upstream http://127.0.0.1:8080\n</code></pre> <p>Running the command above will print out the instructions to connect to the tunnel:</p> <pre><code># Access your tunnel via ClusterIP: acmeco.tunnels\ninlets-pro uplink client \\\n--url=wss://uplink.example.com/tunnels/acmeco \\\n--upstream=http://127.0.0.1:8080 \\\n--token=z4oubxcamiv89V0dy8ytmjUEPwAmY0yFyQ6uaBmXsIQHKtAzlT3PcGZRgK\n</code></pre> <p>Run a container in the cluster to check the file server is accessible through the http tunnel using curl: <code>curl -i acmeco.tunnels:8000</code></p> <pre><code>$ kubectl run -t -i curl --rm \\\n--image ghcr.io/openfaas/curl:latest /bin/sh   \n\n$ curl -i acmeco.tunnels:8000\nHTTP/1.1 200 OK\nContent-Type: text/html; charset=utf-8\nDate: Thu, 17 Nov 2022 08:39:48 GMT\nLast-Modified: Mon, 14 Nov 2022 20:52:53 GMT\nContent-Length: 973\n\n&lt;pre&gt;\n&lt;a href=\"README.md\"&gt;README.md&lt;/a&gt;\n&lt;/pre&gt;\n</code></pre>"},{"location":"uplink/create-tunnels/#how-to-tunnel-multiple-http-services-from-a-customer","title":"How to tunnel multiple HTTP services from a customer","text":"<p>The following example shows how to access more than one HTTP service over the same tunnel. It is possible to expose multiple upstream services over a single tunnel.</p> <p>Start a tunnel client and add multiple upstreams:</p> <pre><code>inlets-pro uplink client \\\n--url wss://uplink.example.com/tunnels/acmeco \\\n--upstream prometheus=http://127.0.0.1:9090 \\\n--upstream gateway=http://127.0.0.1:8080 \\\n--token-file ./token.txt\n</code></pre> <p>Access both services using <code>curl</code>:</p> <pre><code>$ kubectl run -t -i curl --rm \\\n--image ghcr.io/openfaas/curl:latest /bin/sh   \n\n$ curl -i -H \"Host: prometheus\" acmeco.tunnels:8000\nHTTP/1.1 302 Found\nContent-Length: 29\nContent-Type: text/html; charset=utf-8\nDate: Thu, 16 Feb 2023 16:29:09 GMT\nLocation: /graph\n\n&lt;a href=\"/graph\"&gt;Found&lt;/a&gt;.\n\n\n$ curl -i -H \"Host: gateway\" acmeco.tunnels:8000\nHTTP/1.1 301 Moved Permanently\nContent-Length: 39\nContent-Type: text/html; charset=utf-8\nDate: Thu, 16 Feb 2023 16:29:11 GMT\nLocation: /ui/\n\n&lt;a href=\"/ui/\"&gt;Moved Permanently&lt;/a&gt;.\n</code></pre> <p>Note that the <code>Host</code> header has to be set in the request so the tunnel knows which upstream to send the request to.</p>"},{"location":"uplink/create-tunnels/#tunnel-a-customers-tcp-service","title":"Tunnel a customer's TCP service","text":"<p>Perhaps you need to access a customer's Postgres database from their private network?</p>"},{"location":"uplink/create-tunnels/#create-a-tcp-tunnel-using-a-custom-resource","title":"Create a TCP tunnel using a Custom Resource","text":"<p>Example Custom Resource to deploy a tunnel for acmeco\u2019s production Postgres database:</p> <pre><code>apiVersion: uplink.inlets.dev/v1alpha1\nkind: Tunnel\nmetadata:\nname: prod-database\nnamespace: acmeco\nspec:\nlicenseRef:\nname: inlets-uplink-license\nnamespace: acmeco\ntcpPorts:\n- 5432\n</code></pre> <p>Alternatively the cli can be used to create a new tunnel:</p> <pre><code>inlets-pro tunnel create prod-database \\\n-n acmeco\n  --port 5432\n</code></pre>"},{"location":"uplink/create-tunnels/#run-postgresql-on-your-private-server","title":"Run postgresql on your private server","text":"<p>The quickest way to spin up a Postgres instance on your own machine would be to use Docker:</p> <pre><code>head -c 16 /dev/urandom |shasum \n8cb3efe58df984d3ab89bcf4566b31b49b2b79b9\n\nexport PASSWORD=\"8cb3efe58df984d3ab89bcf4566b31b49b2b79b9\"\n\ndocker run --rm --name postgres \\\n-p 5432:5432 \\\n-e POSTGRES_PASSWORD=8cb3efe58df984d3ab89bcf4566b31b49b2b79b9 \\\n-ti postgres:latest\n</code></pre>"},{"location":"uplink/create-tunnels/#connect-with-an-inlets-uplink-client","title":"Connect with an inlets uplink client","text":"<pre><code>export UPLINK_DOMAIN=\"uplink.example.com\"\n\ninlets-pro uplink client \\\n--url wss://${UPLINK_DOMAIN}/acmeco/prod-database \\\n--upstream 127.0.0.1:5432 \\\n--token-file ./token.txt\n</code></pre>"},{"location":"uplink/create-tunnels/#access-the-customer-database-from-within-kubernetes","title":"Access the customer database from within Kubernetes","text":"<p>Now that the tunnel is established, you can connect to the customer's Postgres database from within Kubernetes using its ClusterIP <code>prod-database.acmeco.svc.cluster.local</code>:</p> <p>Try it out:</p> <pre><code>export PASSWORD=\"8cb3efe58df984d3ab89bcf4566b31b49b2b79b9\"\n\nkubectl run -i -t psql \\\n--env PGPORT=5432 \\\n--env PGPASSWORD=$PASSWORD --rm \\\n--image postgres:latest -- psql -U postgres -h prod-database.acmeco\n</code></pre> <p>Try a command such as <code>CREATE database websites (url TEXT)</code>, <code>\\dt</code> or <code>\\l</code>.</p>"},{"location":"uplink/create-tunnels/#getting-help","title":"Getting help","text":"<p>Feel free to reach out to our team via email for technical support.</p>"},{"location":"uplink/ingress-for-tunnels/","title":"Ingress for tunnels","text":"<p>Info</p> <p>Inlets Uplink is designed to connect customer services to a remote Kubernetes cluster for command and control as part of a SaaS product.</p> <p>Any tunnelled service can be accessed directly from within the cluster and does not need to be exposed to the public Internet for access.</p> <p>Beware: by following these instructions, you are exposing one or more of those tunnels to the public Internet.</p> <p>Make inlets uplink HTTP tunnels publicly accessible by setting up ingress for the data plane.</p> <p>The instructions assume that you want to expose two HTTP tunnels. We will configure ingress for the first tunnel, called <code>grafana</code>, on the domain <code>grafana.example.com</code>. The second tunnel, called <code>openfaas</code>, will use the domain <code>openfaas.example.com</code>.</p> <p>Both tunnels can be created with <code>kubectl</code> or the <code>inlets-pro</code> cli. See create tunnels for more info:</p> kubectlcli <pre><code>$ cat &lt;&lt;EOF | kubectl apply -f - \napiVersion: uplink.inlets.dev/v1alpha1\nkind: Tunnel\nmetadata:\n  name: grafana\n  namespace: tunnels\nspec:\n  licenseRef:\n    name: inlets-uplink-license\n    namespace: tunnels\n---\napiVersion: uplink.inlets.dev/v1alpha1\nkind: Tunnel\nmetadata:\n  name: openfaas\n  namespace: tunnels\nspec:\n  licenseRef:\n    name: inlets-uplink-license\n    namespace: tunnels\nEOF\n</code></pre> <pre><code>$ inlets-pro tunnel create grafana\nCreated tunnel openfaas. OK.\n\n$ inlets-pro tunnel create openfaas\nCreated tunnel openfaas. OK.\n</code></pre> <p>Follow the instruction for Kubernetes Ingress or Istio depending on how you deployed inlets uplink.</p>"},{"location":"uplink/ingress-for-tunnels/#setup-tunnel-ingress","title":"Setup tunnel ingress","text":"<ol> <li> <p>Create a new certificate Issuer for tunnels:</p> <pre><code>export EMAIL=\"you@example.com\"\n\ncat &gt; tunnel-issuer-prod.yaml &lt;&lt;EOF\napiVersion: cert-manager.io/v1\nkind: Issuer\nmetadata:\n  name: tunnels-letsencrypt-prod\n  namespace: inlets\nspec:\n  acme:\n    server: https://acme-v02.api.letsencrypt.org/directory\n    email: $EMAIL\n    privateKeySecretRef:\n    name: tunnels-letsencrypt-prod\n    solvers:\n    - http01:\n        ingress:\n          class: \"nginx\"\nEOF\n</code></pre> </li> <li> <p>Create an ingress resource for the tunnel:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\nname: grafana-tunnel-ingress\nnamespace: inlets\nannotations:\nkubernetes.io/ingress.class: nginx\ncert-manager.io/issuer: tunnels-letsencrypt-prod\nspec:\nrules:\n- host: grafana.example.com\nhttp:\npaths:\n- path: /\npathType: Prefix\nbackend:\nservice:\nname: grafana.tunnels\nport:\nnumber: 8000\ntls:\n- hosts:\n- grafana.example.com\nsecretName: grafana-cert\n</code></pre> <p>Note that the annotation <code>cert-manager.io/issuer</code> is used to reference the certificate issuer created in the first step.</p> </li> </ol> <p>To setup ingress for multiple tunnels simply define multiple ingress resources. For example apply a second ingress resource for the openfaas tunnel:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\nname: openfaas-tunnel-ingress\nnamespace: inlets\nannotations:\nkubernetes.io/ingress.class: nginx\ncert-manager.io/issuer: tunnels-letsencrypt-prod\nspec:\nrules:\n- host: openfaas.example.com\nhttp:\npaths:\n- path: /\npathType: Prefix\nbackend:\nservice:\nname: openfaas.tunnels\nport:\nnumber: 8000\ntls:\n- hosts:\n- openfaas.example.com\nsecretName: openfaas-cert\n</code></pre>"},{"location":"uplink/ingress-for-tunnels/#setup-tunnel-ingress-with-an-istio-ingress-gateway","title":"Setup tunnel ingress with an Istio Ingress gateway","text":"<ol> <li> <p>Create a new certificate Issuer for tunnels:</p> <pre><code>export EMAIL=\"you@example.com\"\n\ncat &gt; tunnel-issuer-prod.yaml &lt;&lt;EOF\napiVersion: cert-manager.io/v1\nkind: Issuer\nmetadata:\n  name: tunnels-letsencrypt-prod\n  namespace: istio-system\nspec:\n  acme:\n    server: https://acme-v02.api.letsencrypt.org/directory\n    email: $EMAIL\n    privateKeySecretRef:\n      name: tunnels-letsencrypt-prod\n    solvers:\n    - http01:\n        ingress:\n          class: \"istio\"\nEOF\n</code></pre> <p>We are using the Let's Encrypt production server which has strict limits on the API. A staging server is also available at https://acme-staging-v02.api.letsencrypt.org/directory. If you are creating a lot of certificates while testing it would be better to use the staging server.</p> </li> <li> <p>Create a new certificate resource. In this case we want to expose two tunnels on their own domain, <code>grafana.example.com</code> and <code>openfaas.example.com</code>. This will require two certificates, one for each domain:</p> <pre><code>apiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\nname: grafana-cert\nnamespace: istio-system\nspec:\nsecretName: grafana-cert\ncommonName: grafana.example.com\ndnsNames:\n- grafana.example.com\nissuerRef:\nname: tunnels-letsencrypt-prod\nkind: Issuer\n\n---\napiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\nname: openfaas-cert\nnamespace: istio-system\nspec:\nsecretName: openfaas-cert\ncommonName: openfaas.example.com\ndnsNames:\n- openfaas.example.com\nissuerRef:\nname: tunnels-letsencrypt-prod\nkind: Issuer\n</code></pre> <p>Note that both the certificates and issuer are created in the <code>istio-system</code> namespace.</p> </li> <li> <p>Configure the ingress gateway for both tunnels. In this case we create a single resource for both hosts but you could also split the configuration into multiple Gateway resources.</p> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: Gateway\nmetadata:\nname: tunnel-gateway\nnamespace: inlets\nspec:\nselector:\nistio: ingressgateway # use Istio default gateway implementation\nservers:\n- port:\nnumber: 443\nname: https\nprotocol: HTTPS  tls:\nmode: SIMPLE\ncredentialName: grafana-cert\nhosts:\n- grafana.example.com\n- port:\nnumber: 443\nname: https\nprotocol: HTTPS\ntls:\nmode: SIMPLE\ncredentialName: openfaas-cert\nhosts:\n- openfaas.example.com\n</code></pre> <p>Note that the <code>credentialsName</code> references the secrets for the certificates created in the previous step.</p> </li> <li> <p>Configure the gateway's traffic routes by defining corresponding virtual services:</p> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\nname: grafana\nnamespace: inlets\nspec:\nhosts:\n- grafana.example.com\ngateways:\n- tunnel-gateway\nhttp:\n- match:\n- uri:\nprefix: /\nroute:\n- destination:\nhost: grafana.tunnels.svc.cluster.local\nport:\nnumber: 8000\n---\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\nname: openfaas\nnamespace: inlets\nspec:\nhosts:\n- openfaas.example.com\ngateways:\n- tunnel-gateway\nhttp:\n- match:\n- uri:\nprefix: /\nroute:\n- destination:\nhost: openfaas.tunnels.svc.cluster.local\nport:\nnumber: 8000\n</code></pre> </li> </ol> <p>After applying these resources you should be able to access the data plane for both tunnels on their custom domain.</p>"},{"location":"uplink/manage-tunnels/","title":"Manage customer tunnels","text":"<p>You can use <code>kubectl</code> or the tunnel plugin for the <code>inlets-pro</code> CLI to manage tunnels.</p>"},{"location":"uplink/manage-tunnels/#list-tunnels","title":"List tunnels","text":"<p>List tunnels across all namespaces:</p> kubectlcli <pre><code>$ kubectl get tunnels -A\n\nNAMESPACE     NAME         AUTHTOKENNAME   DEPLOYMENTNAME   TCP PORTS   DOMAINS\ntunnels       acmeco       acmeco          acmeco           [8080]      \ncustomer1     ssh          ssh             ssh              [50035]\ncustomer1     prometheus   prometheus      prometheus       []         [prometheus.customer1.example.com]\n</code></pre> <pre><code>$ inlets-pro tunnel list -A\n\nTUNNEL     DOMAINS                              PORTS   CREATED\nacmeco     []                                   [8080]  2022-11-22 11:51:35 +0100 CET\nssh        []                                   [50035] 2022-11-24 18:19:01 +0100 CET\nprometheus [prometheus.customer1.example.com]   []      2022-11-24 11:43:23 +0100 CET\n</code></pre> <p>To list the tunnels within a namespace:</p> kubectlcli <pre><code>$ kubectl get tunnels -n customer1\n\nNAME         AUTHTOKENNAME   DEPLOYMENTNAME   TCP PORTS   DOMAINS\nssh          ssh             ssh              [50035]\n</code></pre> <pre><code>$ inlets-pro tunnel list -n customer1\n\nTUNNEL     DOMAINS   PORTS   CREATED\nssh        []        [50035] 2022-11-22 11:51:35 +0100 CET\n</code></pre>"},{"location":"uplink/manage-tunnels/#delete-a-tunnel","title":"Delete a tunnel","text":"<p>Deleting a tunnel will remove all resources for the tunnel.</p> <p>To remove a tunnel run:</p> kubectlcli <pre><code>kubectl delete -n tunnels \\\ntunnel/acmeco </code></pre> <pre><code>inlets-pro tunnel remove acmeco \\\n-n tunnels\n</code></pre> <p>Do also remember to stop the customer's inlets uplink client.</p>"},{"location":"uplink/manage-tunnels/#update-the-ports-or-domains-for-a-tunnel","title":"Update the ports or domains for a tunnel","text":"<p>You can update a tunnel and configure its TCP ports or domain names by editing the Tunnel Custom Resource:</p> <pre><code>kubectl edit -n tunnels \\\ntunnel/acmeco  </code></pre> <p>Imagine you wanted to add port 8081, when you already had port 8080 exposed:</p> <pre><code>apiVersion: uplink.inlets.dev/v1alpha1\nkind: Tunnel\nmetadata:\n name: acmeco\n  namespace: tunnels\nspec:\n licenseRef:\n    name: inlets-uplink-license\n    namespace: tunnels\n  tcpPorts:\n  - 8080\n+ - 8081\n</code></pre> <p>Alternatively, if you have the tunnel saved as a YAML file, you can edit it and apply it again with <code>kubectl apply</code>.</p>"},{"location":"uplink/manage-tunnels/#check-the-logs-of-a-tunnel","title":"Check the logs of a tunnel","text":"<p>The logs for tunnels can be useful for troubleshooting or to see if clients are connecting successfully.</p> <p>Get the logs for a tunnel deployment: </p> <pre><code>$ kubectl logs -n tunnels deploy/acmeco -f\n\n2022/11/22 12:07:38 Inlets Uplink For SaaS &amp; Service Providers (Inlets Uplink for 5x Customers)\n2022/11/22 12:07:38 Licensed to: user@example.com\ninlets (tm) uplink server\nAll rights reserved OpenFaaS Ltd (2022)\n\nMetrics on: 0.0.0.0:8001\nControl-plane on: 0.0.0.0:8123\nHTTP data-plane on: 0.0.0.0:8000\ntime=\"2022/11/22 12:33:34\" level=info msg=\"Added upstream: * =&gt; http://127.0.0.1:9090 (9355de15c687471da9766cbe51423e54)\"\ntime=\"2022/11/22 12:33:34\" level=info msg=\"Handling backend connection request [9355de15c687471da9766cbe51423e54]\"\n</code></pre>"},{"location":"uplink/manage-tunnels/#rotate-the-secret-for-a-tunnel","title":"Rotate the secret for a tunnel","text":"<p>You may want to rotate a secret for a customer if you think the secret has been leaked. The token can be rotated manually using <code>kubectl</code> or with a single command using the <code>tunnel</code> CLI plugin.</p> kubectlcli <p>Delete the token secret. The default secret has the same name as the tunnel. The inlets uplink controller will automatically create a new secret.</p> <pre><code>kubectl delete -n tunnels \\\nsecret/acmeco </code></pre> <p>The tunnel has to be restarted to use the new token. </p> <pre><code>kubectl rollout restart -n tunnels \\\ndeploy/acmeco\n</code></pre> <p>Rotate the tunnel token:</p> <pre><code>inlets-pro tunnel rotate acmeco \\\n-n tunnels\n</code></pre> <p>Any connected tunnels will disconnect at this point, and won\u2019t be able to reconnect until you configure them with the updated token.</p> <p>Retrieve the new token for the tunnel and save it to a file:</p> kubectlcli <pre><code>kubectl get -n tunnels secret/acmeco \\\n-o jsonpath=\"{.data.token}\" | base64 --decode &gt; token.txt </code></pre> <pre><code>inlets-pro tunnel token acmeco \\\n-n tunnels &gt; token.txt\n</code></pre> <p>The contents will be saved in <code>token.txt</code></p>"},{"location":"uplink/monitoring-tunnels/","title":"Monitoring inlets uplink","text":"<p>Inlets Uplink comes with an integrated Prometheus deployment that automatically collects metrics for each tunnel.</p> <p>Note</p> <p>Prometheus is deployed with Inlets Uplink by default. If you don't need monitoring you can disable it in the <code>values.yaml</code> of the Inlets Uplink Helm chart:</p> <pre><code>prometheus:\ncreate: false\n</code></pre> <p>You can explore the inlets data using Prometheus's built-in expression browser. To access it, port forward the prometheus service and than navigate to http://localhost:9090/graph</p> <pre><code>kubectl port-forward \\\n-n inlets \\\nsvc/prometheus 9090:9090\n</code></pre>"},{"location":"uplink/monitoring-tunnels/#metrics-for-the-control-plane","title":"Metrics for the control-plane","text":"<p>The control-plane metrics can give you insights into the number of clients that are connected and the number of http requests made to the control-plane endpoint for each tunnel.</p> Metric Type Description Labels controlplane_connected_gauge gauge gauge of inlets clients connected to the control plane <code>tunnel_name</code> controlplane_requests_total counter total HTTP requests processed by connecting clients on the control plane <code>code</code>, <code>tunnel_name</code>"},{"location":"uplink/monitoring-tunnels/#metrics-for-the-data-plane","title":"Metrics for the data-plane","text":"<p>The data-plane metrics can give you insights in the services that are exposed through your tunnel.</p> Metric Type Description Labels dataplane_connections_gauge gauge gauge of connections established over data plane <code>port</code>, <code>type</code>, <code>tunnel_name</code> dataplane_connections_total counter total count of connections established over data plane <code>port</code>, <code>type</code>, <code>tunnel_name</code> dataplane_requests_total counter total HTTP requests processed <code>code</code>, <code>host</code>, <code>method</code>, <code>tunnel_name</code> dataplane_request_duration_seconds histogram seconds spent serving HTTP requests <code>code</code>, <code>host</code>, <code>method</code>, <code>tunnel_name</code>, <p>The connections metrics show the number of connections that are open at this point in time, and on which ports. The <code>type</code> label indicates whether the connection is for a <code>http</code> or <code>tcp</code> upstream.</p> <p>The request metrics only include HTTP upstreams. These metrics can be used to get Rate, Error, Duration (RED) information for any API or website that is connected through the tunnel.</p>"},{"location":"uplink/monitoring-tunnels/#setup-grafana-for-monitoring","title":"Setup Grafana for monitoring","text":"<p>Grafana can be used to visualize the data collected by the inlets uplink Prometheus instance. We provide a sample dashboard that you can use as a starting point.</p> <p> </p> <p>Inlets uplink Grafana dashboard</p> <p>The dashboard can help you get insights in:</p> <ul> <li>The number of client connected to each tunnel.</li> <li>Invocation to the control plane for each tunnel. This can help with detecting misbehaving clients.</li> <li>Rate, Error, Duration (RED) information for HTTP tunnels.</li> <li>The number of connections TCP connections opened for each tunnel.</li> </ul>"},{"location":"uplink/monitoring-tunnels/#install-grafana","title":"Install Grafana","text":"<p>There are three options we recommend for getting access to Grafana.</p> <ul> <li>Grafana installed with its Helm chart</li> <li>Grafana Cloud</li> <li>AWS managed Grafana</li> </ul> <p>You can install Grafana in one line with arkade:</p> <p><pre><code>arkade install grafana\n</code></pre> Grafana can also be installed with Helm. See: Grafana Helm Chart</p> <p>Port forward grafana and retrieve the admin password to login:</p> <pre><code># Expose the service via port-forward:\nkubectl --namespace grafana port-forward service/grafana 3000:80\n\n# Get the admin password:\nkubectl get secret --namespace grafana grafana -o jsonpath=\"{.data.admin-password}\" | base64 --decode ; echo\n</code></pre> <p>Access Grafana on http://127.0.0.1:3000 and login as admin.</p>"},{"location":"uplink/monitoring-tunnels/#add-a-data-source","title":"Add a data source","text":"<p>Before you import the dashboard, you need to add the inlets-uplink prometheus instance as a data source:</p> <ol> <li>Select the cog icon on the side menu to show the configuration options.</li> <li> <p>Select Data sources.</p> <p>This opens the data sources page, which displays a list of previously configured data sources for the Grafana instance.</p> </li> <li> <p>Select Add data source and pick Prometheus from the list of supported data sources.</p> </li> <li> <p>Configure the inlets Prometheus instance as a data source:</p> <p></p> <ul> <li>In the name field set: <code>inlets-prometheus</code></li> <li>For the URL use: <code>http://prometheus.inlets:9090</code> <p>if you installed inlets uplink in a different namespace this url should be <code>http://prometheus.&lt;namespace&gt;:9090</code></p> </li> <li>Set the scrape interval field to <code>30s</code></li> </ul> </li> </ol>"},{"location":"uplink/monitoring-tunnels/#import-the-dashboard","title":"Import the dashboard","text":"<p>Import the inlets uplink dashboard in Grafana:</p> <ol> <li>Click Dashboards &gt; Import in the side menu.</li> <li>Copy the dashboard JSON text</li> <li> <p>Paste the dashboard JSON into the text area.</p> <p></p> </li> </ol>"},{"location":"uplink/overview/","title":"Inlets Uplink overview","text":"<p>What's the difference between Inlets Pro and Inlets Uplink?</p> <p>Inlets Pro is a stand-alone binary that can be use to expose local HTTPs and TCP services on a remote machine or network.</p> <p>Inlets Uplink is a complete management solution for tunnels for SaaS companies and service providers. It's designed for scale, multi-tenancy and easy management.</p> <p>Inlets Uplink is our answer to the question: \"How do you access customer services from within your own product?\"</p> <p>You may consider building your own agent, using a AWS SQS queue, or a VPN.</p> <p>The first two options involve considerable work both up front and in the long run. VPNs require firewall changes, specific network conditions, and lengthy paperwork.</p> <p>Inlets Uplink uses a TLS encrypted websocket to make an outbound connection, and can also work over corporate HTTP proxies.</p> <p>Here are some of the other differences between Inlets Pro and Inlets Uplink:</p> <ul> <li>The management solution is built-in, self-hosted and runs on your Kubernetes cluster</li> <li>You can create a tunnel almost instantly via CLI, REST API or the \"Tunnel\" Custom Resource</li> <li>The license is installed on the server, instead of each client needing it</li> <li>TCP ports can be remapped to avoid conflicts</li> <li>A single tunnel can expose HTTP and TCP at the same time</li> <li>All tunnels can be monitored centrally for reliability and usage</li> <li>By default all tunnels are private and only available for access by your own applications</li> </ul> <p>With Uplink, you deploy tunnel servers for a customers to your Kubernetes cluster, and our operator takes care of everything else.</p> <p>You can read more about why we created inlets uplink in the product announcement.</p>"},{"location":"uplink/overview/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Become a provider</li> <li>Create a tunnel</li> <li>Connect to a tunnel</li> <li>Manage tunnels</li> <li>Expose tunnels publicly (optional)</li> <li>Monitor tunnels</li> </ul> <p>You can reach out to us if you have questions: Contact the inlets team</p>"}]}